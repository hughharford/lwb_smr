{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "580044b7-8c16-4e71-9f3d-7512c617c72e",
   "metadata": {},
   "source": [
    "# TO SUMMARISE:\n",
    "\n",
    "## We have worked night and day to be ready, streamlit etc\n",
    "## Now to run any plausibly good model through the same test, and view the results.\n",
    "\n",
    "## Also, run the models through the same predict set to see their outputs and pick the best 'looker'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f001de34-89c9-438b-a0ba-ca4c9ecf46e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#\n",
    "# Doing these postcodes, \n",
    "# it is clear the NEW model works better (less Buckingham Palace car park) etc\n",
    "\n",
    "# ########################      \n",
    "#    LWB_SMR__POSTCODES_TO_USE\n",
    "# ########################      \n",
    "\n",
    "# Suburban London (HH)        SE23 3YL \n",
    "\n",
    "# Buckingham Palace           SW1A 1AA\n",
    "\n",
    "# Edinburgh Castle            EH1 2NG\n",
    "\n",
    "# Glasgow                     G3 8LZ\n",
    "\n",
    "# Le Wagon                    E2 8DY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba62a50-482d-49cf-aff7-ad2180018ac5",
   "metadata": {},
   "source": [
    "### grabbing models from various places..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a105e57c-f039-4345-b590-580f87cfc627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GSTUIL COMMANDS TO DOWNLOAS VARIOUS MODELS INTO THE CWD:\n",
    "\n",
    "# model names include the date the run was started on (might run overnight...) \n",
    "\n",
    "# gsutil cp gs://lwb-solar-my-roof/models/2206var_hh_vertex_models.zip 2206var_hh_vertex_models.zip\n",
    "# gsutil cp gs://lwb-solar-my-roof/models/aa_vertex_220616_day8_model_unfreeze_from_params.zip aa_vertex_220616_day8_model_unfreeze_from_params.zip \n",
    "# gsutil cp gs://lwb-solar-my-roof/models/220615_josh_checkpoint_VGG16_Combo_Dice_BCE_unfrozen_block.h5 220615_josh_checkpoint_VGG16_Combo_Dice_BCE_unfrozen_block.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3a1a365-4988-400e-92fb-70ef8b2ab7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names_dict ={\n",
    "    'm1_old': '220612_Josh_model_vertexAI_07_FULL_dataset_dice.h5',\n",
    "    'm2_new': '220615_josh_checkpoint_VGG16_Combo_Dice_BCE_unfrozen_block.h5'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b543fd79-822a-4870-826b-898debad855e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('220612_Josh_model_vertexAI_07_FULL_dataset_dice.h5',\n",
       " '220615_josh_checkpoint_VGG16_Combo_Dice_BCE_unfrozen_block.h5')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_names_dict['m1_old'],model_names_dict['m2_new'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c942a20-9b3f-4303-9a83-4e141510352f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-01 15:27:48.347700: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-07-01 15:27:48.347725: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "# from sklearn.model_selection import train_test_split\n",
    "from lwb_smr.data import GetData, LoadDataSets # for: create_tensor_slicer, process_data_inc_autotune\n",
    "from lwb_smr.params import BATCH_SIZE, EPOCHS, LOSS\n",
    "from lwb_smr.params import EXPERIMENT_NAME, EXPERIMENT_TAGS\n",
    "from lwb_smr.params import TEST_RUN, DATA_AUGMENTATION\n",
    "\n",
    "LOAD_MODEL_FROM = 'local' # or set to: 'gcp', or 'new_arch'\n",
    "\n",
    "MLFLOW_URI = \"https://mlflow.lewagon.ai/\"\n",
    "UNET_INPUT_SHAPE = (224,224,3)\n",
    "\n",
    "# for these values, see params.py\n",
    "EXPERIMENT_NAME = EXPERIMENT_NAME\n",
    "EXPERIMENT_TAGS = EXPERIMENT_TAGS\n",
    "\n",
    "DATA_AUGMENTATION = DATA_AUGMENTATION\n",
    "LOSS = LOSS\n",
    "\n",
    "BATCH_SIZE = BATCH_SIZE \n",
    "EPOCHS = EPOCHS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e9ef3e8-114e-4ace-a647-afee8c5639dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "where_the_data_went.txt\n"
     ]
    }
   ],
   "source": [
    "# run this cell to confirm the raw_data path:\n",
    "\n",
    "!ls '../../raw_data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc387c0a-6a6e-41e4-ab40-626c9d88723d",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = '../../raw_data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23f70336-3247-4c37-95a7-32c62cfb4ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "# GetDat input parameters #\n",
    "###########################\n",
    "\n",
    "train_path = root_path + \"train_RGB_tiles_jpeg/\"\n",
    "mask_path = root_path + \"train_mask_tiles_jpeg/\"\n",
    "# we don't need a test_path, do we?? mask_path = root_path + \"train_mask_tiles_jpeg/\"\n",
    "\n",
    "input_image_size = (250,250)\n",
    "train_percent = 0.7\n",
    "validation_percent = 0.25\n",
    "test_percent = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26115a86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST_RUN in params.py is set to: True\n",
      "BATCH_SIZE = 8 and EPOCHS = 2\n"
     ]
    }
   ],
   "source": [
    "get_data = GetData(train_path,mask_path,train_percent,validation_percent,test_percent)\n",
    "\n",
    "\n",
    "print(f'TEST_RUN in params.py is set to: {TEST_RUN}')\n",
    "print(f'BATCH_SIZE = {BATCH_SIZE} and EPOCHS = {EPOCHS}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b937d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom loss functions:\n",
    "\n",
    "# [note, also defined in SMR_Model() below]\n",
    "\n",
    "def dice_loss(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.math.sigmoid(y_pred)\n",
    "    numerator = 2 * tf.reduce_sum(y_true * y_pred)\n",
    "    denominator = tf.reduce_sum(y_true + y_pred)\n",
    "\n",
    "    return 1 - numerator / denominator\n",
    "\n",
    "def loss_combo_dice_bce(y_true, y_pred):\n",
    "    # JACK\n",
    "    def dice_loss(y_true, y_pred):\n",
    "        y_pred = tf.math.sigmoid(y_pred)\n",
    "        numerator = 2 * tf.reduce_sum(y_true * y_pred)\n",
    "        denominator = tf.reduce_sum(y_true + y_pred)\n",
    "\n",
    "        return 1 - numerator/denominator\n",
    "\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    o = tf.nn.sigmoid_cross_entropy_with_logits(y_true, y_pred) + dice_loss(y_true, y_pred)\n",
    "\n",
    "    return tf.reduce_mean(o)\n",
    "\n",
    "custom_objects_dict = {\n",
    "                'LOSS': 'dice_loss',\n",
    "}\n",
    "\n",
    "custom_objects_dict_OLD = {\n",
    "                'dice_loss': 'dice_loss',\n",
    "}\n",
    "\n",
    "custom_objects_dict_NEW = {\n",
    "                'loss_combo_dice_bce': 'loss_combo_dice_bce'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426f1cde",
   "metadata": {},
   "source": [
    "# LOAD OF MODEL from GCP if you need:\n",
    "## set the path... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee9b9855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOT THIS ONE!!!!   \n",
    "#               LOADING_MODEL_GSUTIL = 'gs://lwb-solar-my-roof/models/220610_full_model_vgg16_10e_20e_more.h5'\n",
    "# THIS ONE >>>>>>> \n",
    "#               i.e. with the dice loss\n",
    "if LOAD_MODEL_FROM == 'gcp':\n",
    "    LOADING_MODEL_GSUTIL = 'gs://lwb-solar-my-roof/models/220611_VGG16_Dice_20e_in_shape_224x224x3.h5'\n",
    "    loaded_model = tf.keras.models.load_model(LOADING_MODEL_GSUTIL, custom_objects=custom_objects_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d695a2a",
   "metadata": {},
   "source": [
    "# LOCAL LOAD OF MODEL:\n",
    "## set the path... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eaf78e1a-dc68-481f-b744-2ab922b28545",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('220612_Josh_model_vertexAI_07_FULL_dataset_dice.h5',\n",
       " '220615_josh_checkpoint_VGG16_Combo_Dice_BCE_unfrozen_block.h5')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_names_dict['m1_old'], model_names_dict['m2_new']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "26def907",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "No file or directory found at /home/hsth/code/hughharford/lwb_smr/raw_data/models/220612_Josh_model_vertexAI_07_FULL_dataset_dice.h5",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_75666/1452074139.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mLOAD_MODEL_FROM\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'local'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mloadedOLD_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodelOLD_load_string\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects_dict_OLD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mloadedNEW_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodelNEW_load_string\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects_dict_NEW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/lewagon/lib/python3.8/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/lewagon/lib/python3.8/site-packages/keras/saving/save.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'No file or directory found at {filepath_str}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: No file or directory found at /home/hsth/code/hughharford/lwb_smr/raw_data/models/220612_Josh_model_vertexAI_07_FULL_dataset_dice.h5"
     ]
    }
   ],
   "source": [
    "# WORKS WELL FOR LOCAL LOAD:\n",
    "# model_load_string = 'input_your_local_path_to_model_here'\n",
    "model_local_path = '/home/hsth/code/hughharford/lwb_smr/raw_data/models/'\n",
    "modelOLD_load_string = model_local_path + model_names_dict['m1_old']\n",
    "modelNEW_load_string = model_local_path + model_names_dict['m2_new']\n",
    "\n",
    "if LOAD_MODEL_FROM == 'local':\n",
    "    loadedOLD_model = tf.keras.models.load_model(modelOLD_load_string, custom_objects=custom_objects_dict_OLD)\n",
    "    loadedNEW_model = tf.keras.models.load_model(modelNEW_load_string, custom_objects=custom_objects_dict_NEW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dfcb8eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check its architecture\n",
    "# loaded_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16831487-80ab-4cb7-a2af-be149f762077",
   "metadata": {},
   "source": [
    "# OR CREATE A NEW MODEL WITH UNFROZEN LAYERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3fa09574-15e4-46f1-8150-ea399c93e5d3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if LOAD_MODEL_FROM == 'new_arch':\n",
    "    from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Activation, ReLU\n",
    "    from tensorflow.keras.layers import BatchNormalization, Conv2DTranspose, Concatenate\n",
    "    from tensorflow.keras.models import Model, Sequential\n",
    "    from tensorflow.keras.utils import plot_model\n",
    "    import tensorflow as tf\n",
    "\n",
    "    from tensorflow.keras.applications import VGG16\n",
    "\n",
    "    class SMR_Model_unfrozen():\n",
    "        ''' creating vgg16 model that last block is trainable\n",
    "        '''\n",
    "\n",
    "        def __init__(self, input_shape, model_path_to_file=''):\n",
    "            self.input_shape = input_shape\n",
    "            if model_path_to_file:\n",
    "                self.model_to_load = model_path_to_file\n",
    "\n",
    "        def convolution_block(self, inputs, num_filters):\n",
    "            ''' simple UNET convolution block with BatchNormalisation '''\n",
    "\n",
    "            # convolution layer 1 of the block\n",
    "            x = Conv2D(num_filters, (3,3), padding='same')(inputs)  # padding='same' to avoid cut-down with conv\n",
    "            x = BatchNormalization()(x)\n",
    "            x = Activation('relu')(x)\n",
    "\n",
    "            # convolution layer 2 of the block\n",
    "            x = Conv2D(num_filters, (3,3), padding='same')(x)\n",
    "            x = BatchNormalization()(x)\n",
    "            x = Activation('relu')(x)\n",
    "\n",
    "            # max pooling not used here as just the bridge\n",
    "\n",
    "            return x\n",
    "\n",
    "        def decoder_block(self, inputs, skip_tensor, num_filters):\n",
    "            ''' decoder block for UNET '''\n",
    "            # adds in the skips with concatenate\n",
    "            x = Conv2DTranspose(num_filters, (2,2), strides=2, padding='same')(inputs) # stride important here to up-sample\n",
    "            x = Concatenate()([x, skip_tensor])     # bringing in skip layer\n",
    "            x = self.convolution_block(x, num_filters)\n",
    "\n",
    "            return x\n",
    "\n",
    "        def build_vgg16_unet(self):\n",
    "            ''' build vgg-16 '''\n",
    "\n",
    "            inputs = Input(self.input_shape)\n",
    "\n",
    "            # see actual VGG-16 here: https://github.com/keras-team/keras/blob/v2.9.0/keras/applications/vgg16.py#L43-L227\n",
    "            vgg16 = VGG16(include_top=False, weights='imagenet', input_tensor=inputs)\n",
    "            # vgg16.summary()\n",
    "            # Unfreeze the last block of VGG16\n",
    "            for layer in vgg16.layers[:15]:\n",
    "                layer.trainable = False\n",
    "\n",
    "            # check that layers have been set accordingly:\n",
    "            for i, layer in enumerate(vgg16.layers):\n",
    "                print(i, layer.name, layer.trainable)\n",
    "\n",
    "\n",
    "            ''' Encoder - skip layers '''\n",
    "            skip1 = vgg16.get_layer('block1_conv2').output #  256 x 256, 64 filters in vgg16\n",
    "            skip2 = vgg16.get_layer('block2_conv2').output #  128 x 128, 128 filters in vgg16\n",
    "            skip3 = vgg16.get_layer('block3_conv3').output #   64 x 64, 256 filters in vgg16\n",
    "            skip4 = vgg16.get_layer('block4_conv3').output #   32 x 32, 512 filters in vgg16\n",
    "            # display('skip4: ' + str(skip4.shape))\n",
    "\n",
    "            # only need to specify the skip layers, as VGG16 is an Encoder\n",
    "            # Therefore, VGG16 comes built with MaxPool2d, so we don't specify\n",
    "\n",
    "            ''' Bridge '''\n",
    "            bridge = vgg16.get_layer('block5_conv3').output # 16 x 16, with 512 filters in vgg16\n",
    "            # display('bridge: ' + str(bridge.shape))\n",
    "\n",
    "\n",
    "            ''' Decoder '''\n",
    "            d1 = self.decoder_block(bridge, skip4, 512) #  512 filters, as per the bridge\n",
    "            d2 = self.decoder_block(d1, skip3, 256) #  256 filters\n",
    "            d3 = self.decoder_block(d2, skip2, 128) #  128 filters\n",
    "            d4 = self.decoder_block(d3, skip1, 64)  #   64 filters\n",
    "\n",
    "            ''' Output '''\n",
    "            outputs = Conv2D(1, (1,1), padding='same', activation='sigmoid')(d4)\n",
    "\n",
    "            model = Model(inputs, outputs, name='VGG16_UNET_LastBlockUnfrozen_ComboLossDICE_BCE')\n",
    "\n",
    "            return model\n",
    "\n",
    "\n",
    "        def loss_combo_dice_bce(self, y_true, y_pred):\n",
    "            # JACK\n",
    "            def dice_loss(y_true, y_pred):\n",
    "                y_pred = tf.math.sigmoid(y_pred)\n",
    "                numerator = 2 * tf.reduce_sum(y_true * y_pred)\n",
    "                denominator = tf.reduce_sum(y_true + y_pred)\n",
    "\n",
    "                return 1 - numerator/denominator\n",
    "\n",
    "            y_true = tf.cast(y_true, tf.float32)\n",
    "            o = tf.nn.sigmoid_cross_entropy_with_logits(y_true, y_pred) + dice_loss(y_true, y_pred)\n",
    "\n",
    "            return tf.reduce_mean(o)\n",
    "\n",
    "        def compile_model(self, m):\n",
    "            ''' with accuracy, binaryIoU, AuC '''\n",
    "            # metrics\n",
    "            threshold = 0.5\n",
    "            binaryIoU = tf.keras.metrics.BinaryIoU(target_class_ids=[1], threshold=threshold)\n",
    "            AuC = tf.keras.metrics.AUC()\n",
    "\n",
    "            # loss\n",
    "            #self.dice_loss = ...\n",
    "\n",
    "            # Compile Model\n",
    "            m.compile(\n",
    "                        loss=self.loss_combo_dice_bce,\n",
    "                        optimizer='adam',\n",
    "                        metrics=['accuracy', binaryIoU, AuC]\n",
    "                        )\n",
    "            return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ef2dc70f-f720-4b00-a19c-276b07d76e05",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# get_vgg16 = SMR_Model_unfrozen(UNET_INPUT_SHAPE)\n",
    "# model_vgg16 = get_vgg16.build_vgg16_unet()\n",
    "# model_vgg16_compiled = get_vgg16.compile_model(model_vgg16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2953242e-3256-4696-8438-e0267a710ff1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# history = model_vgg16_compiled.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2d074b-ef96-4cd9-a984-d8ae94a895e9",
   "metadata": {},
   "source": [
    "# TENSOR SLICE DATA LOADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1f1bfa94-84a3-4fcf-9cb1-dfed162302d6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## NOW TRYING: create_tensor_slicer\n",
    "# \n",
    "\n",
    "# x_path = '../../../raw_data/train_RGB_tiles_jpeg/'\n",
    "# x_images = os.listdir(x_path)\n",
    "# y_path = '../../../raw_data/train_mask_tiles_jpeg/'\n",
    "# y_masks = os.listdir(y_path)\n",
    "\n",
    "# root_path = '../../raw_data/'\n",
    "# folders = ['train_RGB_tiles_jpeg', 'train_mask_tiles_jpeg']\n",
    "# folder_path = [f'{root_path}{folder}' for folder in folders]\n",
    "# folder_path\n",
    "\n",
    "# train_images, train_mask = [], []\n",
    "# train_images =[f'../../../raw_data/train_RGB_tiles_jpeg/{filename}' for filename in os.listdir(folder_path[0])]\n",
    "# # for i, filename in enumerate(os.listdir(folder_path[0])):\n",
    "# #     if i == 2*BATCH_SIZE: break\n",
    "# #     train_images.append(f'../../../raw_data/train_RGB_tiles_jpeg/{filename}')\n",
    "    \n",
    "# train_mask = [f'../../../raw_data/train_mask_tiles_jpeg/{filename}' for filename in os.listdir(folder_path[1])]\n",
    "# # for i, filename in enumerate(os.listdir(folder_path[1])):\n",
    "# #     if i == 2*BATCH_SIZE: break\n",
    "# #     train_mask.append(f'../../../raw_data/train_mask_tiles_jpeg/{filename}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "95e5a4c1-57e8-4dad-90f4-58f9ec6edcef",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## NOW TRYING: create_tensor_slicer\n",
    "# \n",
    "# # train_images.sort()\n",
    "# train_mask.sort()\n",
    "\n",
    "# train_df = pd.DataFrame()\n",
    "# # train_df['file_path'] = train_images\n",
    "# train_df['image_path'] = train_images\n",
    "# train_df['mask_path'] = train_mask\n",
    "\n",
    "# len(train_df), train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "20b3769d-24d9-4378-97e2-28ae1ee6e243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "13d2c2e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets match, proceed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['train_x', 'train_y', 'val_x', 'val_y', 'test_x', 'test_y'])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dict = LoadDataSets(\n",
    "    root_path + \"image_datasets_csv/train_dataset.csv\",\n",
    "    root_path + \"image_datasets_csv/validation_dataset.csv\",\n",
    "    root_path + \"image_datasets_csv/test_dataset.csv\").load_datasets()\n",
    "data_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "237149c5-1be0-4939-a96e-b7709a28fef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train, ds_val, ds_test = get_data.create_tensor_slicer(data_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "28454a51-42e3-4c29-bd0c-d6753b756596",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TensorSliceDataset element_spec=(TensorSpec(shape=(), dtype=tf.string, name=None), TensorSpec(shape=(), dtype=tf.string, name=None))>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4f7ed35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process_data_inc_autotune(self, ds_train, ds_val=None, ds_test=None,\n",
    "#                                   data_augmentation=1):\n",
    "ds_train, ds_val, ds_test = get_data.process_data_inc_autotune(\n",
    "    ds_train, ds_val, ds_test, data_augmentation=DATA_AUGMENTATION)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0f4f81d0-26f5-48ec-bea0-e8ec4ed47581",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset element_spec=(TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 224, 224, 1), dtype=tf.float32, name=None))>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7925488-670a-4d53-97e9-fde6788e6c0d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1ab6d800-39ea-4458-bdcb-abff179bd1ed",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Activation, ReLU\n",
    "from tensorflow.keras.layers import BatchNormalization, Conv2DTranspose, Concatenate\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "from tensorflow.keras.applications import VGG16\n",
    "\n",
    "class SMR_Model():\n",
    "    ''' creating our first lwb_smr models '''\n",
    "\n",
    "    def __init__(self, input_shape):\n",
    "        self.input_shape = input_shape\n",
    "\n",
    "    def get_latest_model(self):\n",
    "        model = self.build_vgg16_unet(self.input_shape)\n",
    "        model = self.compile_model(model)\n",
    "\n",
    "        return model\n",
    "\n",
    "    def convolution_block(self, inputs, num_filters):\n",
    "        ''' simple UNET convolution block with BatchNormalisation '''\n",
    "\n",
    "        # convolution layer 1 of the block\n",
    "        x = Conv2D(num_filters, (3,3), padding='same')(inputs)  # padding='same' to avoid cut-down with conv\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('relu')(x)\n",
    "\n",
    "        # convolution layer 2 of the block\n",
    "        x = Conv2D(num_filters, (3,3), padding='same')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('relu')(x)\n",
    "\n",
    "        # max pooling not used here as just the bridge\n",
    "\n",
    "        return x\n",
    "\n",
    "    def decoder_block(self, inputs, skip_tensor, num_filters):\n",
    "        ''' decoder block for UNET '''\n",
    "        # adds in the skips with concatenate\n",
    "        x = Conv2DTranspose(num_filters, (2,2), strides=2, padding='same')(inputs) # stride important here to up-sample\n",
    "        x = Concatenate()([x, skip_tensor])     # bringing in skip layer\n",
    "        x = self.convolution_block(x, num_filters)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def build_vgg16_unet(self, input_shape):\n",
    "        ''' build vgg-16 '''\n",
    "\n",
    "        inputs = Input(input_shape)\n",
    "\n",
    "        # see actual VGG-16 here: https://github.com/keras-team/keras/blob/v2.9.0/keras/applications/vgg16.py#L43-L227\n",
    "        vgg16 = VGG16(include_top=False, weights='imagenet', input_tensor=inputs)\n",
    "        # vgg16.summary()\n",
    "        vgg16.trainable = False\n",
    "\n",
    "        ''' Encoder - skip layers '''\n",
    "        skip1 = vgg16.get_layer('block1_conv2').output #  256 x 256, 64 filters in vgg16\n",
    "        skip2 = vgg16.get_layer('block2_conv2').output #  128 x 128, 128 filters in vgg16\n",
    "        skip3 = vgg16.get_layer('block3_conv3').output #   64 x 64, 256 filters in vgg16\n",
    "        skip4 = vgg16.get_layer('block4_conv3').output #   32 x 32, 512 filters in vgg16\n",
    "        # display('skip4: ' + str(skip4.shape))\n",
    "\n",
    "        # only need to specify the skip layers, as VGG16 is an Encoder\n",
    "        # Therefore, VGG16 comes built with MaxPool2d, so we don't specify\n",
    "\n",
    "        ''' Bridge '''\n",
    "        bridge = vgg16.get_layer('block5_conv3').output # 16 x 16, with 512 filters in vgg16\n",
    "        # display('bridge: ' + str(bridge.shape))\n",
    "\n",
    "\n",
    "        ''' Decoder '''\n",
    "        d1 = self.decoder_block(bridge, skip4, 512) #  512 filters, as per the bridge\n",
    "        d2 = self.decoder_block(d1, skip3, 256) #  256 filters\n",
    "        d3 = self.decoder_block(d2, skip2, 128) #  128 filters\n",
    "        d4 = self.decoder_block(d3, skip1, 64)  #   64 filters\n",
    "\n",
    "        ''' Output '''\n",
    "        outputs = Conv2D(1, (1,1), padding='same', activation='sigmoid')(d4)\n",
    "\n",
    "        model = Model(inputs, outputs, name='first_VGG16_UNET')\n",
    "\n",
    "        return model\n",
    "    \n",
    "    def dice_loss(self, y_true, y_pred):\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        y_pred = tf.math.sigmoid(y_pred)\n",
    "        numerator = 2 * tf.reduce_sum(y_true * y_pred)\n",
    "        denominator = tf.reduce_sum(y_true + y_pred)\n",
    "\n",
    "        return 1 - numerator / denominator\n",
    "\n",
    "    def loss_combo_dice_bce(self, y_true, y_pred):\n",
    "        # JACK\n",
    "        def dice_loss(y_true, y_pred):\n",
    "            y_pred = tf.math.sigmoid(y_pred)\n",
    "            numerator = 2 * tf.reduce_sum(y_true * y_pred)\n",
    "            denominator = tf.reduce_sum(y_true + y_pred)\n",
    "\n",
    "            return 1 - numerator/denominator\n",
    "\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        o = tf.nn.sigmoid_cross_entropy_with_logits(y_true, y_pred) + dice_loss(y_true, y_pred)\n",
    "\n",
    "        return tf.reduce_mean(o)\n",
    "\n",
    "    def compile_model(self, m):\n",
    "        ''' with accuracy, binaryIoU, AuC '''\n",
    "        # metrics\n",
    "        threshold = 0.5\n",
    "        binaryIoU = tf.keras.metrics.BinaryIoU(target_class_ids=[1], threshold=threshold)\n",
    "        AuC = tf.keras.metrics.AUC()\n",
    "\n",
    "        # loss\n",
    "        #self.dice_loss = ...\n",
    "        \n",
    "        # Compile Model\n",
    "        m.compile(\n",
    "                    loss=self.dice_loss,\n",
    "                    optimizer='adam',\n",
    "                    metrics=['accuracy', binaryIoU, AuC]\n",
    "                    )\n",
    "        return m\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e02df14-1730-4a67-84d9-6df7918d7cfe",
   "metadata": {
    "tags": []
   },
   "source": [
    "# utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c9f96beb-091b-4ebf-815a-26b02d7ea818",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# mlflow\n",
    "#\n",
    "# and others...\n",
    "\n",
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "from memoized_property import memoized_property\n",
    "\n",
    "MLFLOW_URI = \"https://mlflow.lewagon.ai/\"\n",
    "\n",
    "class PushMLFlow():\n",
    "    '''\n",
    "        MLFLOW_URI = \"https://mlflow.lewagon.ai/\"\n",
    "        EXPERIMENT_NAME = \"[UK] [LONDON] [SOLAR_ROOF] TEST RUN\" # template\n",
    "        EXPERIMENT_TAGS = {\n",
    "            'USER': 'TEST_user',\n",
    "            'RUN NAME': 'TEST_evaluation of models',\n",
    "            'VERSION': '1.0.1',\n",
    "            'LOSS': 'dice'\n",
    "            'DESCRIPTION': 'testing MLFlow Pipeline. Model - basic U-Net structure, 2 epochs, 15 images'\n",
    "        }\n",
    "    '''\n",
    "\n",
    "    def __init__(self, experiment_name, experiment_tags):\n",
    "        self.experiment_name = experiment_name\n",
    "        self.experiment_tag = experiment_tags\n",
    "\n",
    "    @memoized_property\n",
    "    def mlflow_client(self):\n",
    "        mlflow.set_tracking_uri(MLFLOW_URI)\n",
    "        return MlflowClient()\n",
    "\n",
    "    @memoized_property\n",
    "    def mlflow_experiment_id(self):\n",
    "        try:\n",
    "            return self.mlflow_client.create_experiment(self.experiment_name)\n",
    "        except BaseException:\n",
    "            return self.mlflow_client.get_experiment_by_name(self.experiment_name).experiment_id\n",
    "\n",
    "    @memoized_property\n",
    "    def mlflow_run(self):\n",
    "        return self.mlflow_client.create_run(self.mlflow_experiment_id, tags=self.experiment_tag)\n",
    "\n",
    "    def mlflow_log_param(self, key, value):\n",
    "        self.mlflow_client.log_param(self.mlflow_run.info.run_id, key, value)\n",
    "\n",
    "    def mlflow_log_metric(self, key, value):\n",
    "        self.mlflow_client.log_metric(self.mlflow_run.info.run_id, key, value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3d60ba85-ce10-431b-b21a-dd584f8a4a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6352a612-b695-41e5-83a0-828ecdd13dc8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "------SETTING FOR DATA RUN------\n",
      "--------------------------------------------------------------------------------\n",
      "------MODEL RUNNING------\n",
      "================================================================================\n",
      "------MODEL RUN SUCCESFULLY COMPLETED------\n",
      "--------------------------------------------------------------------------------\n",
      "------MODEL EVALUATING------\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "in user code:\n\n    File \"/home/hsth/.pyenv/versions/3.8.12/envs/lewagon/lib/python3.8/site-packages/keras/engine/training.py\", line 1557, in test_function  *\n        return step_function(self, iterator)\n    File \"/home/hsth/.pyenv/versions/3.8.12/envs/lewagon/lib/python3.8/site-packages/keras/engine/training.py\", line 1546, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/hsth/.pyenv/versions/3.8.12/envs/lewagon/lib/python3.8/site-packages/keras/engine/training.py\", line 1535, in run_step  **\n        outputs = model.test_step(data)\n    File \"/home/hsth/.pyenv/versions/3.8.12/envs/lewagon/lib/python3.8/site-packages/keras/engine/training.py\", line 1501, in test_step\n        self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/home/hsth/.pyenv/versions/3.8.12/envs/lewagon/lib/python3.8/site-packages/keras/engine/training.py\", line 948, in compute_loss\n        return self.compiled_loss(\n    File \"/home/hsth/.pyenv/versions/3.8.12/envs/lewagon/lib/python3.8/site-packages/keras/engine/compile_utils.py\", line 201, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/home/hsth/.pyenv/versions/3.8.12/envs/lewagon/lib/python3.8/site-packages/keras/losses.py\", line 139, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/home/hsth/.pyenv/versions/3.8.12/envs/lewagon/lib/python3.8/site-packages/keras/losses.py\", line 243, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n\n    TypeError: 'str' object is not callable\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_49829/3532818414.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m     \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_49829/3532818414.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'------MODEL RUN SUCCESFULLY COMPLETED------'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_49829/3532818414.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m80\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m'-'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'------MODEL EVALUATING------'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMFLOW\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlflow_log_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'metric X val output'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.12/envs/lewagon/lib/python3.8/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.12/envs/lewagon/lib/python3.8/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtf__test_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: in user code:\n\n    File \"/home/hsth/.pyenv/versions/3.8.12/envs/lewagon/lib/python3.8/site-packages/keras/engine/training.py\", line 1557, in test_function  *\n        return step_function(self, iterator)\n    File \"/home/hsth/.pyenv/versions/3.8.12/envs/lewagon/lib/python3.8/site-packages/keras/engine/training.py\", line 1546, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/hsth/.pyenv/versions/3.8.12/envs/lewagon/lib/python3.8/site-packages/keras/engine/training.py\", line 1535, in run_step  **\n        outputs = model.test_step(data)\n    File \"/home/hsth/.pyenv/versions/3.8.12/envs/lewagon/lib/python3.8/site-packages/keras/engine/training.py\", line 1501, in test_step\n        self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/home/hsth/.pyenv/versions/3.8.12/envs/lewagon/lib/python3.8/site-packages/keras/engine/training.py\", line 948, in compute_loss\n        return self.compiled_loss(\n    File \"/home/hsth/.pyenv/versions/3.8.12/envs/lewagon/lib/python3.8/site-packages/keras/engine/compile_utils.py\", line 201, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/home/hsth/.pyenv/versions/3.8.12/envs/lewagon/lib/python3.8/site-packages/keras/losses.py\", line 139, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/home/hsth/.pyenv/versions/3.8.12/envs/lewagon/lib/python3.8/site-packages/keras/losses.py\", line 243, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n\n    TypeError: 'str' object is not callable\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "# from lwb_smr.CustomDataLoader import CustomDataLoader\n",
    "# from lwb_smr.model import SMR_Model\n",
    "# from lwb_smr.utils import PushMLFlow\n",
    "\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "# from tensorflow.keras.metrics import AUC, IoU\n",
    "\n",
    "        \n",
    "def dice_loss(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.math.sigmoid(y_pred)\n",
    "    numerator = 2 * tf.reduce_sum(y_true * y_pred)\n",
    "    denominator = tf.reduce_sum(y_true + y_pred)\n",
    "\n",
    "    return 1 - numerator / denominator\n",
    "\n",
    "class Trainer():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def set_model(self, loss=dice_loss):\n",
    "        self.loss = loss\n",
    "        # Instantiate Model\n",
    "        # our_input_shape = (224,224,3)\n",
    "\n",
    "        #### getVGG16 = SMR_Model(UNET_INPUT_SHAPE)\n",
    "        #### self.model = getVGG16.get_latest_model()\n",
    "        \n",
    "        # TO RUN A NEWLY CREATED UNFROZEN LAYERS VGG16, USE\n",
    "        # self.model = model_vgg16_compiled\n",
    "        \n",
    "        # TO LOAD FROM ABOVE, USE:\n",
    "        ####                      self.model = loaded_model # see additions above\n",
    "        # self.model = loadedOLD_model\n",
    "        self.model = loadedNEW_model\n",
    "        # see compile in SMR_Model\n",
    "\n",
    "    def start_mlflow(self):\n",
    "        p = PushMLFlow(EXPERIMENT_NAME, EXPERIMENT_TAGS)\n",
    "        return p # returns a class instance of PushMLFlow\n",
    "    \n",
    "    def run(self):\n",
    "\n",
    "        print(80*'-')\n",
    "        print('------SETTING FOR DATA RUN------')\n",
    "\n",
    "        ### \n",
    "        ##\n",
    "        #\n",
    "        # customdata = self.just_get_the_data_loaded()\n",
    "\n",
    "        print(80*'-')\n",
    "        print('------MODEL RUNNING------')\n",
    "\n",
    "        # set mflow      \n",
    "        self.MFLOW = self.start_mlflow() # class instance of MLFLOW\n",
    "\n",
    "        \n",
    "        # set model\n",
    "        self.set_model()\n",
    "\n",
    "        # mc = ModelCheckpoint('220611_checkpoint_VGG16_Dice.h5', save_best_only=True) # could put path here\n",
    "        # es = EarlyStopping(patience=15, restore_best_weights=True)\n",
    "        # self.history = self.model.fit(\n",
    "        #     ds_train,\n",
    "        #     validation_data=ds_val,\n",
    "        #     batch_size=BATCH_SIZE,\n",
    "        #     epochs=EPOCHS,\n",
    "        #     callbacks=[mc, es]\n",
    "        #     )\n",
    "\n",
    "        model_path_and_filename = 'day10evaluate_OLD_model.h5'\n",
    "        self.model.save(model_path_and_filename)\n",
    "\n",
    "        self.loss_n_metrics = ['old model: loss', 'accuracy', 'binaryIoU', 'AuC']\n",
    "        # self.MFLOW.mlflow_log_param('loss', self.loss)\n",
    "        self.MFLOW.mlflow_log_param('accuracy', 'accuracy')\n",
    "        self.MFLOW.mlflow_log_param('binaryIoU', 'binaryIoU')\n",
    "        self.MFLOW.mlflow_log_param('AuC', 'AuC')\n",
    "\n",
    "\n",
    "        print(80*'=')\n",
    "        print('------MODEL RUN SUCCESFULLY COMPLETED------')\n",
    "\n",
    "        self.evaluate()\n",
    "\n",
    "    def evaluate(self):\n",
    "        print(80*'-')\n",
    "        print('------MODEL EVALUATING------')\n",
    "        results = self.model.evaluate(ds_test)\n",
    "        for result in results:\n",
    "            self.MFLOW.mlflow_log_metric('metric X val output', result)\n",
    "        print(80*'=')\n",
    "        print('------MODEL EVALUATED------')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    pass\n",
    "    t = Trainer()\n",
    "    t.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633bd30b-3127-4f24-8ade-052e58ad8051",
   "metadata": {},
   "outputs": [],
   "source": [
    "########\n",
    "####                        DICE             DICE             DICE             DICE             DICE             DICE             DICE             DICE\n",
    "###\n",
    "#\n",
    "#        NOTES: with DICE LOSS driving the model now (this is a sample run. \n",
    "#                                                                          Again, clear that binaryIoU metric is showing increases, as with all other metrics.\n",
    "#                                              (val) DICE LOSS reducing\n",
    "#                                              (val) ACCURACY increasing\n",
    "#                                              (val) binaryIoU increasing\n",
    "#                                              (val) AUC increasing\n",
    "#\n",
    "##\n",
    "###\n",
    "####\n",
    "########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6fd1ff-e898-4563-8f26-d622e1b26976",
   "metadata": {},
   "outputs": [],
   "source": [
    "t.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00435da1-80f0-48dd-ba29-8dc7c83ffb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "t.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab874103-2e4e-4d67-ad07-a47900ea8a5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07cb58b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4c324a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906d6acb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0a781d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1856eb50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a8b5ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-8.m93",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-8:m93"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "eea5debcc8192b4558c87bf2908d31bc8924eb4e6e7ae46e8978999378aeaa55"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
