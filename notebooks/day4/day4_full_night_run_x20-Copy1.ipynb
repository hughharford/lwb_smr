{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3bc97d18-1eb2-4e52-b175-ecc71a9de403",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "274eed32-788a-465a-967b-ec34fe9d9815",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_path = '../../raw_data/train_RGB_tiles_jpeg/'\n",
    "x_images = os.listdir(x_path)\n",
    "y_path = '../../raw_data/train_mask_tiles_jpeg/'\n",
    "y_masks = os.listdir(y_path)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a7395881-9072-495b-992d-4f3281896c65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../../raw_data/train_RGB_tiles_jpeg', '../../raw_data/train_mask_tiles_jpeg']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root_path = '../../raw_data/'\n",
    "folders = ['train_RGB_tiles_jpeg', 'train_mask_tiles_jpeg']\n",
    "folder_path = [f'{root_path}{folder}' for folder in folders]\n",
    "folder_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f1c83d91-b2c7-4ef2-a256-12fd07740e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images, train_mask = [], []\n",
    "train_images =[f'../../raw_data/train_RGB_tiles_jpeg/{filename}' for filename in os.listdir(folder_path[0])]\n",
    "train_mask = [f'../../raw_data/train_mask_tiles_jpeg/{filename}' for filename in os.listdir(folder_path[1])]\n",
    "\n",
    "train_images.sort()\n",
    "train_mask.sort()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f79d917e-4bb9-4e1d-b11a-7af22ada9891",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_path</th>\n",
       "      <th>mask_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../../raw_data/train_RGB_tiles_jpeg/austin10_x...</td>\n",
       "      <td>../../raw_data/train_mask_tiles_jpeg/austin10_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../../raw_data/train_RGB_tiles_jpeg/austin10_x...</td>\n",
       "      <td>../../raw_data/train_mask_tiles_jpeg/austin10_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>../../raw_data/train_RGB_tiles_jpeg/austin10_x...</td>\n",
       "      <td>../../raw_data/train_mask_tiles_jpeg/austin10_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>../../raw_data/train_RGB_tiles_jpeg/austin10_x...</td>\n",
       "      <td>../../raw_data/train_mask_tiles_jpeg/austin10_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>../../raw_data/train_RGB_tiles_jpeg/austin10_x...</td>\n",
       "      <td>../../raw_data/train_mask_tiles_jpeg/austin10_...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          image_path  \\\n",
       "0  ../../raw_data/train_RGB_tiles_jpeg/austin10_x...   \n",
       "1  ../../raw_data/train_RGB_tiles_jpeg/austin10_x...   \n",
       "2  ../../raw_data/train_RGB_tiles_jpeg/austin10_x...   \n",
       "3  ../../raw_data/train_RGB_tiles_jpeg/austin10_x...   \n",
       "4  ../../raw_data/train_RGB_tiles_jpeg/austin10_x...   \n",
       "\n",
       "                                           mask_path  \n",
       "0  ../../raw_data/train_mask_tiles_jpeg/austin10_...  \n",
       "1  ../../raw_data/train_mask_tiles_jpeg/austin10_...  \n",
       "2  ../../raw_data/train_mask_tiles_jpeg/austin10_...  \n",
       "3  ../../raw_data/train_mask_tiles_jpeg/austin10_...  \n",
       "4  ../../raw_data/train_mask_tiles_jpeg/austin10_...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.DataFrame()\n",
    "# train_df['file_path'] = train_images\n",
    "train_df['image_path'] = train_images\n",
    "train_df['mask_path'] = train_mask\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f0628b26-f29c-46d9-a6ed-002f3876b60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def holdout(df, train_ratio=0.7, test_to_val_ratio=(0.05/0.3), include_all=False):\n",
    "\n",
    "    img_paths = df[\"image_path\"].values\n",
    "    msk_paths = df[\"mask_path\"].values\n",
    "\n",
    "    df_mask = df.copy()\n",
    "\n",
    "    df_train, df_val = train_test_split(df_mask, train_size=train_ratio)\n",
    "    df_test, df_val = train_test_split(df_val, test_size=test_to_val_ratio)\n",
    "\n",
    "    ds_train = tf.data.Dataset.from_tensor_slices(\n",
    "         (df_train[\"image_path\"].values, df_train[\"mask_path\"].values)\n",
    "    )\n",
    "    ds_val = tf.data.Dataset.from_tensor_slices(\n",
    "        (df_val[\"image_path\"].values, df_val[\"mask_path\"].values)\n",
    "    )\n",
    "    ds_test = tf.data.Dataset.from_tensor_slices(\n",
    "        (df_test[\"image_path\"].values, df_test[\"mask_path\"].values)\n",
    "    )\n",
    "\n",
    "    return ds_train, ds_val, ds_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "055e1d2a-2866-4a67-b341-fbe53ebe8dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train, ds_val, ds_test = holdout(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f673a2b4-9431-484d-a73a-4007ec7c6173",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_path(input_path, mask_path):\n",
    "    \"\"\"\n",
    "    Load images from files.\n",
    "    :input_path: the path to the satellite file\n",
    "    :mask_path: the path to the mask file\n",
    "    :return: The image and mask\n",
    "    .. note:: Works with jpg images \n",
    "              Only the first channel is kept for the mask\n",
    "    \"\"\"\n",
    "    \n",
    "    IMAGE_SQ_SIZE = 224\n",
    "\n",
    "    input_img = tf.io.read_file(input_path)   \n",
    "    input_img = tf.io.decode_jpeg(input_img, channels=3)\n",
    "    input_img =  tf.image.resize(input_img, [IMAGE_SQ_SIZE, IMAGE_SQ_SIZE])\n",
    "\n",
    "    mask_img = tf.io.read_file(mask_path)   \n",
    "    mask_img = tf.io.decode_jpeg(mask_img, channels=1)\n",
    "    mask_img =  tf.image.resize(mask_img, [IMAGE_SQ_SIZE, IMAGE_SQ_SIZE])\n",
    "\n",
    "   \n",
    "    return input_img, mask_img\n",
    "\n",
    "def normalize(image, mask):\n",
    "    # image = tf.cast(image, tf.float32) / 255.\n",
    "\n",
    "    return tf.math.divide(image, 255), tf.math.divide(mask, 255) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c7da3fc9-8d6a-4b4c-8bf7-dc42e4f6b2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "ds_train = ds_train.map(process_path) \\\n",
    ".map(normalize) \\\n",
    ".batch(batch_size=60) \\\n",
    ".prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5832baf5-bf32-4191-9560-3fa90f8e9b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "052df7e5-3b1a-4bf2-ad03-ec573c6bc395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_path_and_filename = '../models/first_UNET_input_shape_224x224x3.h5'\n",
    "# model = keras.models.load_model(model_path_and_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e25dd7f8-54e6-4450-aa93-35ef45ee28cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Activation, ReLU\n",
    "from tensorflow.keras.layers import BatchNormalization, Conv2DTranspose, Concatenate\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "from tensorflow.keras.applications import VGG16\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import AUC, IoU\n",
    "\n",
    "\n",
    "\n",
    "class SMR_Model():\n",
    "    ''' creating our first lwb_smr models '''\n",
    "\n",
    "    def __init__(self, input_shape):\n",
    "        self.input_shape = input_shape\n",
    "\n",
    "    def get_latest_model(self):\n",
    "        model = self.build_vgg16_unet(self.input_shape)\n",
    "        model = self.compile_model(model)\n",
    "\n",
    "        return model\n",
    "\n",
    "    def convolution_block(self, inputs, num_filters):\n",
    "        ''' simple UNET convolution block with BatchNormalisation '''\n",
    "\n",
    "        # convolution layer 1 of the block\n",
    "        x = Conv2D(num_filters, (3,3), padding='same')(inputs)  # padding='same' to avoid cut-down with conv\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('relu')(x)\n",
    "\n",
    "        # convolution layer 2 of the block\n",
    "        x = Conv2D(num_filters, (3,3), padding='same')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('relu')(x)\n",
    "\n",
    "        # max pooling not used here as just the bridge\n",
    "\n",
    "        return x\n",
    "\n",
    "    def decoder_block(self, inputs, skip_tensor, num_filters):\n",
    "        ''' decoder block for UNET '''\n",
    "        # adds in the skips with concatenate\n",
    "        x = Conv2DTranspose(num_filters, (2,2), strides=2, padding='same')(inputs) # stride important here to up-sample\n",
    "        x = Concatenate()([x, skip_tensor])     # bringing in skip layer\n",
    "        x = self.convolution_block(x, num_filters)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def build_vgg16_unet(self, input_shape):\n",
    "        ''' build vgg-16 '''\n",
    "\n",
    "        inputs = Input(input_shape)\n",
    "\n",
    "        # see actual VGG-16 here: https://github.com/keras-team/keras/blob/v2.9.0/keras/applications/vgg16.py#L43-L227\n",
    "        vgg16 = VGG16(include_top=False, weights='imagenet', input_tensor=inputs)\n",
    "        # vgg16.summary()\n",
    "        vgg16.trainable = False\n",
    "\n",
    "        ''' Encoder - skip layers '''\n",
    "        skip1 = vgg16.get_layer('block1_conv2').output #  256 x 256, 64 filters in vgg16\n",
    "        skip2 = vgg16.get_layer('block2_conv2').output #  128 x 128, 128 filters in vgg16\n",
    "        skip3 = vgg16.get_layer('block3_conv3').output #   64 x 64, 256 filters in vgg16\n",
    "        skip4 = vgg16.get_layer('block4_conv3').output #   32 x 32, 512 filters in vgg16\n",
    "        # display('skip4: ' + str(skip4.shape))\n",
    "\n",
    "        # only need to specify the skip layers, as VGG16 is an Encoder\n",
    "        # Therefore, VGG16 comes built with MaxPool2d, so we don't specify\n",
    "\n",
    "        ''' Bridge '''\n",
    "        bridge = vgg16.get_layer('block5_conv3').output # 16 x 16, with 512 filters in vgg16\n",
    "        # display('bridge: ' + str(bridge.shape))\n",
    "\n",
    "\n",
    "        ''' Decoder '''\n",
    "        d1 = self.decoder_block(bridge, skip4, 512) #  512 filters, as per the bridge\n",
    "        d2 = self.decoder_block(d1, skip3, 256) #  256 filters\n",
    "        d3 = self.decoder_block(d2, skip2, 128) #  128 filters\n",
    "        d4 = self.decoder_block(d3, skip1, 64)  #   64 filters\n",
    "\n",
    "        ''' Output '''\n",
    "        outputs = Conv2D(1, (1,1), padding='same', activation='sigmoid')(d4)\n",
    "\n",
    "        model = Model(inputs, outputs, name='first_VGG16_UNET')\n",
    "\n",
    "        return model\n",
    "\n",
    "    def compile_model(self, m):\n",
    "        ''' compile as a basic unet for now... first actual run '''\n",
    "        m.compile(\n",
    "            loss='binary_crossentropy',\n",
    "            optimizer='adam'\n",
    "        )\n",
    "        return m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "897c2395-d9af-4567-b13e-e303b6aebfb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "UNET_INPUT_SHAPE = (224,224,3)\n",
    "BATCH_SIZE = 60\n",
    "EPOCHS = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70273942-fd90-47b3-a87d-ab4fab6f3f3c",
   "metadata": {},
   "source": [
    "# RECOMMENDATIONS FOR OVERNIGHT RUN:\n",
    "\n",
    "# epochs = 20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "53cc3873-15fa-408b-a43f-ab148c5849f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.333333333333334"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hours = (EPOCHS*28)/60\n",
    "hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8ada91cf-9e5d-4a7d-9b8f-4220493a9adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-09 21:01:44.545080: W tensorflow/core/common_runtime/bfc_allocator.cc:462] Allocator (GPU_0_bfc) ran out of memory trying to allocate 144.0KiB (rounded to 147456)requested by op Mul\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2022-06-09 21:01:44.545130: I tensorflow/core/common_runtime/bfc_allocator.cc:1010] BFCAllocator dump for GPU_0_bfc\n",
      "2022-06-09 21:01:44.545142: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (256): \tTotal Chunks: 19, Chunks in use: 19. 4.8KiB allocated for chunks. 4.8KiB in use in bin. 1.3KiB client-requested in use in bin.\n",
      "2022-06-09 21:01:44.545150: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (512): \tTotal Chunks: 1, Chunks in use: 1. 512B allocated for chunks. 512B in use in bin. 512B client-requested in use in bin.\n",
      "2022-06-09 21:01:44.545157: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (1024): \tTotal Chunks: 1, Chunks in use: 1. 1.2KiB allocated for chunks. 1.2KiB in use in bin. 1.0KiB client-requested in use in bin.\n",
      "2022-06-09 21:01:44.545163: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (2048): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-06-09 21:01:44.545170: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (4096): \tTotal Chunks: 5, Chunks in use: 3. 33.0KiB allocated for chunks. 20.2KiB in use in bin. 20.2KiB client-requested in use in bin.\n",
      "2022-06-09 21:01:44.545176: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (8192): \tTotal Chunks: 1, Chunks in use: 0. 10.8KiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-06-09 21:01:44.545182: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (16384): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-06-09 21:01:44.545187: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (32768): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-06-09 21:01:44.545192: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (65536): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-06-09 21:01:44.545199: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (131072): \tTotal Chunks: 3, Chunks in use: 2. 418.5KiB allocated for chunks. 288.0KiB in use in bin. 288.0KiB client-requested in use in bin.\n",
      "2022-06-09 21:01:44.545205: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (262144): \tTotal Chunks: 2, Chunks in use: 2. 562.2KiB allocated for chunks. 562.2KiB in use in bin. 432.0KiB client-requested in use in bin.\n",
      "2022-06-09 21:01:44.545212: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (524288): \tTotal Chunks: 1, Chunks in use: 1. 761.0KiB allocated for chunks. 761.0KiB in use in bin. 576.0KiB client-requested in use in bin.\n",
      "2022-06-09 21:01:44.545217: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (1048576): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-06-09 21:01:44.545222: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (2097152): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-06-09 21:01:44.545228: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (4194304): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-06-09 21:01:44.545233: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (8388608): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-06-09 21:01:44.545238: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (16777216): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-06-09 21:01:44.545248: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (33554432): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-06-09 21:01:44.545254: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (67108864): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-06-09 21:01:44.545259: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (134217728): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-06-09 21:01:44.545265: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (268435456): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-06-09 21:01:44.545271: I tensorflow/core/common_runtime/bfc_allocator.cc:1033] Bin for 144.0KiB was 128.0KiB, Chunk State: \n",
      "2022-06-09 21:01:44.545283: I tensorflow/core/common_runtime/bfc_allocator.cc:1039]   Size: 130.5KiB | Requested Size: 0B | in_use: 0 | bin_num: 9, prev:   Size: 6.8KiB | Requested Size: 6.8KiB | in_use: 1 | bin_num: -1, next:   Size: 144.0KiB | Requested Size: 144.0KiB | in_use: 1 | bin_num: -1\n",
      "2022-06-09 21:01:44.545288: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] Next region of size 1835008\n",
      "2022-06-09 21:01:44.545296: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f8416e00000 of size 1280 next 1\n",
      "2022-06-09 21:01:44.545301: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f8416e00500 of size 256 next 2\n",
      "2022-06-09 21:01:44.545306: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f8416e00600 of size 256 next 3\n",
      "2022-06-09 21:01:44.545310: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f8416e00700 of size 256 next 4\n",
      "2022-06-09 21:01:44.545315: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f8416e00800 of size 256 next 5\n",
      "2022-06-09 21:01:44.545319: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f8416e00900 of size 256 next 6\n",
      "2022-06-09 21:01:44.545324: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f8416e00a00 of size 256 next 7\n",
      "2022-06-09 21:01:44.545328: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f8416e00b00 of size 256 next 8\n",
      "2022-06-09 21:01:44.545332: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f8416e00c00 of size 256 next 11\n",
      "2022-06-09 21:01:44.545337: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f8416e00d00 of size 256 next 12\n",
      "2022-06-09 21:01:44.545341: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f8416e00e00 of size 256 next 13\n",
      "2022-06-09 21:01:44.545346: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f8416e00f00 of size 256 next 16\n",
      "2022-06-09 21:01:44.545350: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f8416e01000 of size 256 next 17\n",
      "2022-06-09 21:01:44.545355: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f8416e01100 of size 512 next 20\n",
      "2022-06-09 21:01:44.545359: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f8416e01300 of size 256 next 21\n",
      "2022-06-09 21:01:44.545363: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f8416e01400 of size 256 next 22\n",
      "2022-06-09 21:01:44.545368: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f8416e01500 of size 256 next 23\n",
      "2022-06-09 21:01:44.545374: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7f8416e01600 of size 11008 next 9\n",
      "2022-06-09 21:01:44.545390: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f8416e04100 of size 6912 next 10\n",
      "2022-06-09 21:01:44.545396: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f8416e05c00 of size 256 next 24\n",
      "2022-06-09 21:01:44.545404: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f8416e05d00 of size 256 next 27\n",
      "2022-06-09 21:01:44.545410: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f8416e05e00 of size 256 next 31\n",
      "2022-06-09 21:01:44.545424: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f8416e05f00 of size 256 next 32\n",
      "2022-06-09 21:01:44.545433: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7f8416e06000 of size 6144 next 25\n",
      "2022-06-09 21:01:44.545440: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f8416e07800 of size 6912 next 26\n",
      "2022-06-09 21:01:44.545448: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f8416e09300 of size 280832 next 15\n",
      "2022-06-09 21:01:44.545456: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f8416e4dc00 of size 147456 next 14\n",
      "2022-06-09 21:01:44.545463: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7f8416e71c00 of size 6912 next 30\n",
      "2022-06-09 21:01:44.545470: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f8416e73700 of size 6912 next 29\n",
      "2022-06-09 21:01:44.545476: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7f8416e75200 of size 133632 next 28\n",
      "2022-06-09 21:01:44.545483: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f8416e95c00 of size 147456 next 19\n",
      "2022-06-09 21:01:44.545490: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f8416eb9c00 of size 294912 next 18\n",
      "2022-06-09 21:01:44.545497: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f8416f01c00 of size 779264 next 18446744073709551615\n",
      "2022-06-09 21:01:44.545504: I tensorflow/core/common_runtime/bfc_allocator.cc:1071]      Summary of in-use Chunks by size: \n",
      "2022-06-09 21:01:44.545515: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 19 Chunks of size 256 totalling 4.8KiB\n",
      "2022-06-09 21:01:44.545524: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 512 totalling 512B\n",
      "2022-06-09 21:01:44.545532: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 1280 totalling 1.2KiB\n",
      "2022-06-09 21:01:44.545540: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 3 Chunks of size 6912 totalling 20.2KiB\n",
      "2022-06-09 21:01:44.545549: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 2 Chunks of size 147456 totalling 288.0KiB\n",
      "2022-06-09 21:01:44.545558: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 280832 totalling 274.2KiB\n",
      "2022-06-09 21:01:44.545567: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 294912 totalling 288.0KiB\n",
      "2022-06-09 21:01:44.545575: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 779264 totalling 761.0KiB\n",
      "2022-06-09 21:01:44.545584: I tensorflow/core/common_runtime/bfc_allocator.cc:1078] Sum Total of in-use chunks: 1.60MiB\n",
      "2022-06-09 21:01:44.545592: I tensorflow/core/common_runtime/bfc_allocator.cc:1080] total_region_allocated_bytes_: 1835008 memory_limit_: 1835008 available bytes: 0 curr_region_allocation_bytes_: 3670016\n",
      "2022-06-09 21:01:44.545604: I tensorflow/core/common_runtime/bfc_allocator.cc:1086] Stats: \n",
      "Limit:                         1835008\n",
      "InUse:                         1677312\n",
      "MaxInUse:                      1817088\n",
      "NumAllocs:                          47\n",
      "MaxAllocSize:                   779264\n",
      "Reserved:                            0\n",
      "PeakReserved:                        0\n",
      "LargestFreeBlock:                    0\n",
      "\n",
      "2022-06-09 21:01:44.545617: W tensorflow/core/common_runtime/bfc_allocator.cc:474] ***********xxxxxx**********______*********************************************************xxxxxxxxxx\n",
      "2022-06-09 21:01:44.545738: W tensorflow/core/framework/op_kernel.cc:1733] RESOURCE_EXHAUSTED: failed to allocate memory\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "failed to allocate memory [Op:Mul]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_31730/4262000574.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mgetVGG16\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSMR_Model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mUNET_INPUT_SHAPE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetVGG16\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_vgg16_unet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mUNET_INPUT_SHAPE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_31730/4114610733.py\u001b[0m in \u001b[0;36mbuild_vgg16_unet\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;31m# see actual VGG-16 here: https://github.com/keras-team/keras/blob/v2.9.0/keras/applications/vgg16.py#L43-L227\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0mvgg16\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVGG16\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minclude_top\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'imagenet'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0;31m# vgg16.summary()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mvgg16\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/keras/applications/vgg16.py\u001b[0m in \u001b[0;36mVGG16\u001b[0;34m(include_top, weights, input_tensor, input_shape, pooling, classes, classifier_activation)\u001b[0m\n\u001b[1;32m    146\u001b[0m           img_input)\n\u001b[1;32m    147\u001b[0m   x = layers.Conv2D(\n\u001b[0;32m--> 148\u001b[0;31m       64, (3, 3), activation='relu', padding='same', name='block1_conv2')(x)\n\u001b[0m\u001b[1;32m    149\u001b[0m   \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaxPooling2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'block1_pool'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/keras/backend.py\u001b[0m in \u001b[0;36mrandom_uniform\u001b[0;34m(self, shape, minval, maxval, dtype)\u001b[0m\n\u001b[1;32m   1920\u001b[0m     return tf.random.uniform(\n\u001b[1;32m   1921\u001b[0m         \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mminval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaxval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1922\u001b[0;31m         seed=self.make_legacy_seed())\n\u001b[0m\u001b[1;32m   1923\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1924\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mtruncated_normal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstddev\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: failed to allocate memory [Op:Mul]"
     ]
    }
   ],
   "source": [
    "getVGG16 = SMR_Model(UNET_INPUT_SHAPE)\n",
    "model = getVGG16.build_vgg16_unet(UNET_INPUT_SHAPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5b2417-b195-45d8-a7c3-7b3dcb612d96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ba11ec-71e7-4a84-a3dc-96ac49d16859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# binaryIoU metric\n",
    "threshold = 0.5\n",
    "binaryIoU = tf.keras.metrics.BinaryIoU(target_class_ids=[1], threshold=threshold)\n",
    "AUC = tf.keras.metrics.AUC()\n",
    "\n",
    "# Compile Model\n",
    "model.compile(\n",
    "        loss='binary_crossentropy',\n",
    "        optimizer='adam',\n",
    "        metrics=['accuracy', binaryIoU, AUC]\n",
    "        )\n",
    "\n",
    "mc = ModelCheckpoint('../checkpoints/oxford_segmentation.h5', save_best_only=True) # could put path here\n",
    "es = EarlyStopping(patience=5, restore_best_weights=True)\n",
    "model.fit(\n",
    "    ds_train,\n",
    "    validation_data=ds_val,\n",
    "    batch_size=60,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[mc, es]\n",
    "    )\n",
    "\n",
    "model_path_and_filename = '../models/220609_15epoch_tensor_slices_UNET_input_shape_224x224x3.h5'\n",
    "model.save(model_path_and_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef10772-ca51-40e4-8caa-6313ea281ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.fit(\n",
    "#     ds_train,\n",
    "#     epochs=2,\n",
    "#     batch_size=60,\n",
    "#     validation_data=ds_val\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e05ad2b-0b73-43e5-b3d2-d204f2263e6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-8.m93",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-8:m93"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
