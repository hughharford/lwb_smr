{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb44b59f-3378-4ed0-8c17-f67a78201635",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-09 22:28:03.855160: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-09 22:28:03.865030: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-09 22:28:03.865371: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-09 22:28:03.866272: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-09 22:28:03.868431: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-09 22:28:03.868760: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-09 22:28:03.869020: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-09 22:28:05.022413: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-09 22:28:05.022795: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-09 22:28:05.023125: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-09 22:28:05.023410: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "x_path = '../../raw_data/train_RGB_tiles_jpeg/'\n",
    "x_images = os.listdir(x_path)\n",
    "y_path = '../../raw_data/train_mask_tiles_jpeg/'\n",
    "y_masks = os.listdir(y_path)\n",
    "        \n",
    "\n",
    "root_path = '../../raw_data/'\n",
    "folders = ['train_RGB_tiles_jpeg', 'train_mask_tiles_jpeg']\n",
    "folder_path = [f'{root_path}{folder}' for folder in folders]\n",
    "folder_path\n",
    "\n",
    "train_images, train_mask = [], []\n",
    "train_images =[f'../../raw_data/train_RGB_tiles_jpeg/{filename}' for filename in os.listdir(folder_path[0])]\n",
    "train_mask = [f'../../raw_data/train_mask_tiles_jpeg/{filename}' for filename in os.listdir(folder_path[1])]\n",
    "\n",
    "train_images.sort()\n",
    "train_mask.sort()\n",
    "\n",
    "\n",
    "train_df = pd.DataFrame()\n",
    "# train_df['file_path'] = train_images\n",
    "train_df['image_path'] = train_images\n",
    "train_df['mask_path'] = train_mask\n",
    "\n",
    "train_df.head()\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def holdout(df, train_ratio=0.8, test_to_val_ratio=0.5, include_all=False):\n",
    "\n",
    "    img_paths = df[\"image_path\"].values\n",
    "    msk_paths = df[\"mask_path\"].values\n",
    "\n",
    "    df_mask = df.copy()\n",
    "\n",
    "    df_train, df_val = train_test_split(df_mask, train_size=train_ratio)\n",
    "    df_test, df_val = train_test_split(df_val, test_size=test_to_val_ratio)\n",
    "\n",
    "    ds_train = tf.data.Dataset.from_tensor_slices(\n",
    "         (df_train[\"image_path\"].values, df_train[\"mask_path\"].values)\n",
    "    )\n",
    "    ds_val = tf.data.Dataset.from_tensor_slices(\n",
    "        (df_val[\"image_path\"].values, df_val[\"mask_path\"].values)\n",
    "    )\n",
    "    ds_test = tf.data.Dataset.from_tensor_slices(\n",
    "        (df_test[\"image_path\"].values, df_test[\"mask_path\"].values)\n",
    "    )\n",
    "\n",
    "    return ds_train, ds_val, ds_test\n",
    "\n",
    "ds_train, ds_val, ds_test = holdout(train_df)\n",
    "\n",
    "def process_path(input_path, mask_path):\n",
    "    \"\"\"\n",
    "    Load images from files.\n",
    "    :input_path: the path to the satellite file\n",
    "    :mask_path: the path to the mask file\n",
    "    :return: The image and mask\n",
    "    .. note:: Works with jpg images \n",
    "              Only the first channel is kept for the mask\n",
    "    \"\"\"\n",
    "    \n",
    "    IMAGE_SQ_SIZE = 224\n",
    "\n",
    "    input_img = tf.io.read_file(input_path)   \n",
    "    input_img = tf.io.decode_jpeg(input_img, channels=3)\n",
    "    input_img =  tf.image.resize(input_img, [IMAGE_SQ_SIZE, IMAGE_SQ_SIZE])\n",
    "\n",
    "    mask_img = tf.io.read_file(mask_path)   \n",
    "    mask_img = tf.io.decode_jpeg(mask_img, channels=1)\n",
    "    mask_img =  tf.image.resize(mask_img, [IMAGE_SQ_SIZE, IMAGE_SQ_SIZE])\n",
    "\n",
    "   \n",
    "    return input_img, mask_img\n",
    "\n",
    "def normalize(image, mask):\n",
    "    # image = tf.cast(image, tf.float32) / 255.\n",
    "\n",
    "    return tf.math.divide(image, 255), tf.math.divide(mask, 255) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1cc863df-3e83-447e-823a-d3e50067a40a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "ds_train = ds_train.map(process_path) \\\n",
    ".map(normalize) \\\n",
    ".batch(batch_size=60) \\\n",
    ".prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "ds_val = ds_val.map(process_path) \\\n",
    ".map(normalize) \\\n",
    ".batch(batch_size=60) \\\n",
    ".prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5832baf5-bf32-4191-9560-3fa90f8e9b48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "052df7e5-3b1a-4bf2-ad03-ec573c6bc395",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model_path_and_filename = '../models/first_UNET_input_shape_224x224x3.h5'\n",
    "# model = keras.models.load_model(model_path_and_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e25dd7f8-54e6-4450-aa93-35ef45ee28cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Activation, ReLU\n",
    "from tensorflow.keras.layers import BatchNormalization, Conv2DTranspose, Concatenate\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "from tensorflow.keras.applications import VGG16\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import AUC, IoU\n",
    "\n",
    "\n",
    "\n",
    "class SMR_Model():\n",
    "    ''' creating our first lwb_smr models '''\n",
    "\n",
    "    def __init__(self, input_shape):\n",
    "        self.input_shape = input_shape\n",
    "\n",
    "    def get_latest_model(self):\n",
    "        model = self.build_vgg16_unet(self.input_shape)\n",
    "        model = self.compile_model(model)\n",
    "\n",
    "        return model\n",
    "\n",
    "    def convolution_block(self, inputs, num_filters):\n",
    "        ''' simple UNET convolution block with BatchNormalisation '''\n",
    "\n",
    "        # convolution layer 1 of the block\n",
    "        x = Conv2D(num_filters, (3,3), padding='same')(inputs)  # padding='same' to avoid cut-down with conv\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('relu')(x)\n",
    "\n",
    "        # convolution layer 2 of the block\n",
    "        x = Conv2D(num_filters, (3,3), padding='same')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('relu')(x)\n",
    "\n",
    "        # max pooling not used here as just the bridge\n",
    "\n",
    "        return x\n",
    "\n",
    "    def decoder_block(self, inputs, skip_tensor, num_filters):\n",
    "        ''' decoder block for UNET '''\n",
    "        # adds in the skips with concatenate\n",
    "        x = Conv2DTranspose(num_filters, (2,2), strides=2, padding='same')(inputs) # stride important here to up-sample\n",
    "        x = Concatenate()([x, skip_tensor])     # bringing in skip layer\n",
    "        x = self.convolution_block(x, num_filters)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def build_vgg16_unet(self, input_shape):\n",
    "        ''' build vgg-16 '''\n",
    "\n",
    "        inputs = Input(input_shape)\n",
    "\n",
    "        # see actual VGG-16 here: https://github.com/keras-team/keras/blob/v2.9.0/keras/applications/vgg16.py#L43-L227\n",
    "        vgg16 = VGG16(include_top=False, weights='imagenet', input_tensor=inputs)\n",
    "        # vgg16.summary()\n",
    "        vgg16.trainable = False\n",
    "\n",
    "        ''' Encoder - skip layers '''\n",
    "        skip1 = vgg16.get_layer('block1_conv2').output #  256 x 256, 64 filters in vgg16\n",
    "        skip2 = vgg16.get_layer('block2_conv2').output #  128 x 128, 128 filters in vgg16\n",
    "        skip3 = vgg16.get_layer('block3_conv3').output #   64 x 64, 256 filters in vgg16\n",
    "        skip4 = vgg16.get_layer('block4_conv3').output #   32 x 32, 512 filters in vgg16\n",
    "        # display('skip4: ' + str(skip4.shape))\n",
    "\n",
    "        # only need to specify the skip layers, as VGG16 is an Encoder\n",
    "        # Therefore, VGG16 comes built with MaxPool2d, so we don't specify\n",
    "\n",
    "        ''' Bridge '''\n",
    "        bridge = vgg16.get_layer('block5_conv3').output # 16 x 16, with 512 filters in vgg16\n",
    "        # display('bridge: ' + str(bridge.shape))\n",
    "\n",
    "\n",
    "        ''' Decoder '''\n",
    "        d1 = self.decoder_block(bridge, skip4, 512) #  512 filters, as per the bridge\n",
    "        d2 = self.decoder_block(d1, skip3, 256) #  256 filters\n",
    "        d3 = self.decoder_block(d2, skip2, 128) #  128 filters\n",
    "        d4 = self.decoder_block(d3, skip1, 64)  #   64 filters\n",
    "\n",
    "        ''' Output '''\n",
    "        outputs = Conv2D(1, (1,1), padding='same', activation='sigmoid')(d4)\n",
    "\n",
    "        model = Model(inputs, outputs, name='first_VGG16_UNET')\n",
    "\n",
    "        return model\n",
    "\n",
    "    def compile_model(self, m):\n",
    "        ''' compile as a basic unet for now... first actual run '''\n",
    "        m.compile(\n",
    "            loss='binary_crossentropy',\n",
    "            optimizer='adam'\n",
    "        )\n",
    "        return m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "897c2395-d9af-4567-b13e-e303b6aebfb4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "UNET_INPUT_SHAPE = (224,224,3)\n",
    "BATCH_SIZE = 60\n",
    "EPOCHS = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70273942-fd90-47b3-a87d-ab4fab6f3f3c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# RECOMMENDATIONS FOR OVERNIGHT RUN:\n",
    "\n",
    "# epochs = 15\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "53cc3873-15fa-408b-a43f-ab148c5849f5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.75"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hours = (EPOCHS*27)/60\n",
    "hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8ada91cf-9e5d-4a7d-9b8f-4220493a9adf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "getVGG16 = SMR_Model(UNET_INPUT_SHAPE)\n",
    "model = getVGG16.build_vgg16_unet(UNET_INPUT_SHAPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "62026e24-dee5-41c5-b300-abc06fd87798",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-09 21:16:05.644604: I tensorflow/stream_executor/cuda/cuda_dnn.cc:368] Loaded cuDNN version 8200\n",
      "2022-06-09 21:16:27.041551: W tensorflow/core/common_runtime/bfc_allocator.cc:275] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.92GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n",
      "2022-06-09 21:16:27.041604: W tensorflow/core/common_runtime/bfc_allocator.cc:275] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.92GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n",
      "2022-06-09 21:16:27.230458: W tensorflow/core/common_runtime/bfc_allocator.cc:275] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.46GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n",
      "2022-06-09 21:16:27.230505: W tensorflow/core/common_runtime/bfc_allocator.cc:275] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.46GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184/960 [====>.........................] - ETA: 23:01 - loss: 0.2690 - accuracy: 0.8444 - binary_io_u: 0.4313 - auc: 0.9011"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-09 21:22:09.157955: W tensorflow/core/common_runtime/bfc_allocator.cc:275] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.23GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "360/960 [==========>...................] - ETA: 17:48 - loss: 0.2292 - accuracy: 0.8559 - binary_io_u: 0.4727 - auc: 0.9200"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-09 21:27:22.601013: W tensorflow/core/common_runtime/bfc_allocator.cc:275] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.23GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "436/960 [============>.................] - ETA: 15:33 - loss: 0.2205 - accuracy: 0.8587 - binary_io_u: 0.4813 - auc: 0.9244"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-09 21:29:37.982145: W tensorflow/core/common_runtime/bfc_allocator.cc:275] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.23GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "562/960 [================>.............] - ETA: 11:48 - loss: 0.2094 - accuracy: 0.8621 - binary_io_u: 0.4936 - auc: 0.9301"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-09 21:33:22.483087: W tensorflow/core/common_runtime/bfc_allocator.cc:275] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.23GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "621/960 [==================>...........] - ETA: 10:03 - loss: 0.2056 - accuracy: 0.8631 - binary_io_u: 0.4979 - auc: 0.9321"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-09 21:35:07.681655: W tensorflow/core/common_runtime/bfc_allocator.cc:275] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.23GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "694/960 [====================>.........] - ETA: 7:53 - loss: 0.2014 - accuracy: 0.8643 - binary_io_u: 0.5030 - auc: 0.9345"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-09 21:37:17.831583: W tensorflow/core/common_runtime/bfc_allocator.cc:275] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.23GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "960/960 [==============================] - ETA: 0s - loss: 0.1904 - accuracy: 0.8682 - binary_io_u: 0.5162 - auc: 0.9404WARNING:tensorflow:Model was constructed with shape (None, 224, 224, 3) for input KerasTensor(type_spec=TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name='input_1'), name='input_1', description=\"created by layer 'input_1'\"), but it was called on an input with incompatible shape ().\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/opt/conda/lib/python3.7/site-packages/keras/engine/training.py\", line 1525, in test_function  *\n        return step_function(self, iterator)\n    File \"/opt/conda/lib/python3.7/site-packages/keras/engine/training.py\", line 1514, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/opt/conda/lib/python3.7/site-packages/keras/engine/training.py\", line 1507, in run_step  **\n        outputs = model.test_step(data)\n    File \"/opt/conda/lib/python3.7/site-packages/keras/engine/training.py\", line 1471, in test_step\n        y_pred = self(x, training=False)\n    File \"/opt/conda/lib/python3.7/site-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/opt/conda/lib/python3.7/site-packages/keras/engine/input_spec.py\", line 228, in assert_input_compatibility\n        raise ValueError(f'Input {input_index} of layer \"{layer_name}\" '\n\n    ValueError: Exception encountered when calling layer \"first_VGG16_UNET\" (type Functional).\n    \n    Input 0 of layer \"block1_conv1\" is incompatible with the layer: expected min_ndim=4, found ndim=0. Full shape received: ()\n    \n    Call arguments received:\n      • inputs=tf.Tensor(shape=(), dtype=string)\n      • training=False\n      • mask=None\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_10182/1527134657.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     )\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mautograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1146\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1147\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1148\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/opt/conda/lib/python3.7/site-packages/keras/engine/training.py\", line 1525, in test_function  *\n        return step_function(self, iterator)\n    File \"/opt/conda/lib/python3.7/site-packages/keras/engine/training.py\", line 1514, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/opt/conda/lib/python3.7/site-packages/keras/engine/training.py\", line 1507, in run_step  **\n        outputs = model.test_step(data)\n    File \"/opt/conda/lib/python3.7/site-packages/keras/engine/training.py\", line 1471, in test_step\n        y_pred = self(x, training=False)\n    File \"/opt/conda/lib/python3.7/site-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/opt/conda/lib/python3.7/site-packages/keras/engine/input_spec.py\", line 228, in assert_input_compatibility\n        raise ValueError(f'Input {input_index} of layer \"{layer_name}\" '\n\n    ValueError: Exception encountered when calling layer \"first_VGG16_UNET\" (type Functional).\n    \n    Input 0 of layer \"block1_conv1\" is incompatible with the layer: expected min_ndim=4, found ndim=0. Full shape received: ()\n    \n    Call arguments received:\n      • inputs=tf.Tensor(shape=(), dtype=string)\n      • training=False\n      • mask=None\n"
     ]
    }
   ],
   "source": [
    "# binaryIoU metric\n",
    "threshold = 0.5\n",
    "binaryIoU = tf.keras.metrics.BinaryIoU(target_class_ids=[1], threshold=threshold)\n",
    "AUC = tf.keras.metrics.AUC()\n",
    "\n",
    "# Compile Model\n",
    "model.compile(\n",
    "        loss='binary_crossentropy',\n",
    "        optimizer='adam',\n",
    "        metrics=['accuracy', binaryIoU, AUC]\n",
    "        )\n",
    "\n",
    "model_path_and_filename = '../models/220609_vxx5_pre_NIGHT_RUN_UNET_input_shape_224x224x3.h5'\n",
    "model.save(model_path_and_filename)\n",
    "\n",
    "mc = ModelCheckpoint('../checkpoints/oxford_segmentation.h5', save_best_only=True) # could put path here\n",
    "es = EarlyStopping(patience=5, restore_best_weights=True)\n",
    "model.fit(\n",
    "    ds_train,\n",
    "    validation_data=ds_val,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[mc, es]\n",
    "    )\n",
    "\n",
    "model_path_and_filename = '../models/220609_vxx5_15epoch_tensor_slices_UNET_input_shape_224x224x3.h5'\n",
    "model.save(model_path_and_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6315391-5dcb-4ee6-b5d5-9be6dca2752f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-8.m93",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-8:m93"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
