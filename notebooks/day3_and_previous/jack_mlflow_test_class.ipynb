{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "jack_mlflow_test_class.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "VduajZBmFHlx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mlflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zSqyawquFjmd",
        "outputId": "9bc55d35-0802-46db-84a9-b6d3b0fe98a4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting mlflow\n",
            "  Downloading mlflow-1.26.1-py3-none-any.whl (17.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 17.8 MB 941 kB/s \n",
            "\u001b[?25hRequirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from mlflow) (1.3.0)\n",
            "Requirement already satisfied: requests>=2.17.3 in /usr/local/lib/python3.7/dist-packages (from mlflow) (2.23.0)\n",
            "Collecting gunicorn\n",
            "  Downloading gunicorn-20.1.0-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 8.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from mlflow) (3.17.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from mlflow) (1.21.6)\n",
            "Collecting databricks-cli>=0.8.7\n",
            "  Downloading databricks-cli-0.16.6.tar.gz (62 kB)\n",
            "\u001b[K     |████████████████████████████████| 62 kB 996 kB/s \n",
            "\u001b[?25hRequirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from mlflow) (2022.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from mlflow) (21.3)\n",
            "Requirement already satisfied: sqlparse>=0.3.1 in /usr/local/lib/python3.7/dist-packages (from mlflow) (0.4.2)\n",
            "Collecting alembic\n",
            "  Downloading alembic-1.8.0-py3-none-any.whl (209 kB)\n",
            "\u001b[K     |████████████████████████████████| 209 kB 66.5 MB/s \n",
            "\u001b[?25hCollecting gitpython>=2.1.0\n",
            "  Downloading GitPython-3.1.27-py3-none-any.whl (181 kB)\n",
            "\u001b[K     |████████████████████████████████| 181 kB 53.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: Flask in /usr/local/lib/python3.7/dist-packages (from mlflow) (1.1.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from mlflow) (1.4.1)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.7/dist-packages (from mlflow) (0.4)\n",
            "Requirement already satisfied: importlib-metadata!=4.7.0,>=3.7.0 in /usr/local/lib/python3.7/dist-packages (from mlflow) (4.11.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from mlflow) (1.3.5)\n",
            "Collecting querystring-parser\n",
            "  Downloading querystring_parser-1.2.4-py2.py3-none-any.whl (7.9 kB)\n",
            "Requirement already satisfied: sqlalchemy in /usr/local/lib/python3.7/dist-packages (from mlflow) (1.4.36)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 25.2 MB/s \n",
            "\u001b[?25hCollecting prometheus-flask-exporter\n",
            "  Downloading prometheus_flask_exporter-0.20.2-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.7/dist-packages (from mlflow) (7.1.2)\n",
            "Collecting docker>=4.0.0\n",
            "  Downloading docker-5.0.3-py2.py3-none-any.whl (146 kB)\n",
            "\u001b[K     |████████████████████████████████| 146 kB 59.6 MB/s \n",
            "\u001b[?25hCollecting pyjwt>=1.7.0\n",
            "  Downloading PyJWT-2.4.0-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: oauthlib>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from databricks-cli>=0.8.7->mlflow) (3.2.0)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.7/dist-packages (from databricks-cli>=0.8.7->mlflow) (0.8.9)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from databricks-cli>=0.8.7->mlflow) (1.15.0)\n",
            "Collecting websocket-client>=0.32.0\n",
            "  Downloading websocket_client-1.3.2-py3-none-any.whl (54 kB)\n",
            "\u001b[K     |████████████████████████████████| 54 kB 3.4 MB/s \n",
            "\u001b[?25hCollecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 2.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from gitpython>=2.1.0->mlflow) (4.2.0)\n",
            "Collecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata!=4.7.0,>=3.7.0->mlflow) (3.8.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.17.3->mlflow) (2022.5.18.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.17.3->mlflow) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.17.3->mlflow) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.17.3->mlflow) (2.10)\n",
            "Collecting Mako\n",
            "  Downloading Mako-1.2.0-py3-none-any.whl (78 kB)\n",
            "\u001b[K     |████████████████████████████████| 78 kB 8.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from alembic->mlflow) (5.7.1)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.7/dist-packages (from sqlalchemy->mlflow) (1.1.2)\n",
            "Requirement already satisfied: Werkzeug<2.0,>=0.15 in /usr/local/lib/python3.7/dist-packages (from Flask->mlflow) (1.0.1)\n",
            "Requirement already satisfied: Jinja2<3.0,>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from Flask->mlflow) (2.11.3)\n",
            "Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.7/dist-packages (from Flask->mlflow) (1.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2<3.0,>=2.10.1->Flask->mlflow) (2.0.1)\n",
            "Requirement already satisfied: setuptools>=3.0 in /usr/local/lib/python3.7/dist-packages (from gunicorn->mlflow) (57.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->mlflow) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->mlflow) (2.8.2)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.7/dist-packages (from prometheus-flask-exporter->mlflow) (0.14.1)\n",
            "Building wheels for collected packages: databricks-cli\n",
            "  Building wheel for databricks-cli (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for databricks-cli: filename=databricks_cli-0.16.6-py3-none-any.whl size=112631 sha256=f216cf3a0c063fca5b072a3a8eb5ffb81d1ec87fb6850cc663df723149dabd50\n",
            "  Stored in directory: /root/.cache/pip/wheels/96/c1/f8/d75a22e789ab6a4dff11f18338c3af4360189aa371295cc934\n",
            "Successfully built databricks-cli\n",
            "Installing collected packages: smmap, websocket-client, pyjwt, Mako, gitdb, querystring-parser, pyyaml, prometheus-flask-exporter, gunicorn, gitpython, docker, databricks-cli, alembic, mlflow\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed Mako-1.2.0 alembic-1.8.0 databricks-cli-0.16.6 docker-5.0.3 gitdb-4.0.9 gitpython-3.1.27 gunicorn-20.1.0 mlflow-1.26.1 prometheus-flask-exporter-0.20.2 pyjwt-2.4.0 pyyaml-6.0 querystring-parser-1.2.4 smmap-5.0.0 websocket-client-1.3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install memoized_property"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_8d2L5IBGCJ6",
        "outputId": "7619df07-3966-457a-9732-b064ba7c577d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting memoized_property\n",
            "  Downloading memoized-property-1.0.3.tar.gz (5.0 kB)\n",
            "Building wheels for collected packages: memoized-property\n",
            "  Building wheel for memoized-property (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for memoized-property: filename=memoized_property-1.0.3-py2.py3-none-any.whl size=4198 sha256=1c9f4cca378de00e06d17424cacf3c9cf305e6d4bcf6dd0271060afcc069290a\n",
            "  Stored in directory: /root/.cache/pip/wheels/de/7b/fe/699bb0345131d630549d98185b87309eb5921d460df9f5562d\n",
            "Successfully built memoized-property\n",
            "Installing collected packages: memoized-property\n",
            "Successfully installed memoized-property-1.0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Wc7FaktJFGAA"
      },
      "outputs": [],
      "source": [
        "import mlflow\n",
        "from mlflow.tracking import MlflowClient\n",
        "from memoized_property import memoized_property"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data loading libraries \n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9iYk9x6RFLHF",
        "outputId": "127b7421-f056-4390-fd60-31736f842f22"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model set up libraries \n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Activation, ReLU\n",
        "from tensorflow.keras.layers import BatchNormalization, Conv2DTranspose, Concatenate\n",
        "from tensorflow.keras.models import Model, Sequential \n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "qbK4ctv8GO7F"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "from PIL import Image\n",
        "from tensorflow.keras.preprocessing import image\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "2dpIdrtIGO4v"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Global Variables"
      ],
      "metadata": {
        "id": "2xObHtm2GRnw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ROOT_PATH = 'drive/MyDrive/SOLAR_ROOF/'\n",
        "FOLDERS = ['tiled_test_images', 'tiled_train_images']\n",
        "FOLDER_PATH = [f'{ROOT_PATH}{folder}/*.tif' for folder in FOLDERS]\n",
        "MLFLOW_URI = \"https://mlflow.lewagon.ai/\"\n",
        "EXPERIMENT_NAME = \"[UK] [LONDON] [SOLAR_ROOF] TEST RUN\"\n",
        "\n",
        "IMAGE_SIZE = (160,160)\n",
        "BATCH_SIZE = 8\n",
        "EPOCHS = 2"
      ],
      "metadata": {
        "id": "_2zCmJHqGO2I"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Get Data Function\n",
        "\n",
        "- returns two arrays X, y \n",
        "- `X.shape = (num_images, image_size, image_size, 3)`\n",
        "- `y.shape = (num_images, image_size, image_size, 1)`"
      ],
      "metadata": {
        "id": "kVzUY8w5Ga7E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data(num_images=15):\n",
        "    test_path =[filename for filename in glob.glob(FOLDER_PATH[0])]\n",
        "    train_path = [filename for filename in glob.glob(FOLDER_PATH[1])]\n",
        "    \n",
        "    # limit to 15 images for test\n",
        "    test_path.sort()\n",
        "    train_path.sort()\n",
        "    \n",
        "    test_path = test_path[:num_images]\n",
        "    train_path = train_path[:num_images] \n",
        "\n",
        "    X = [] # Image\n",
        "    y = [] # Mask\n",
        "    \n",
        "    images_X = []\n",
        "    images_y = []\n",
        "    \n",
        "    for filename in train_path:\n",
        "      im = Image.open(filename)\n",
        "      # print(filename)\n",
        "      images_X.append(im)\n",
        "      im_resized = im.resize(IMAGE_SIZE)\n",
        "      X.append(image.img_to_array(im_resized))\n",
        "\n",
        "    for filename in test_path:\n",
        "      im = Image.open(filename)\n",
        "      # print(filename)\n",
        "      images_y.append(im)\n",
        "      im_resized = im.resize(IMAGE_SIZE)\n",
        "      y.append(image.img_to_array(im_resized))\n",
        "    \n",
        "    \n",
        "    X = np.array(X) / 255.\n",
        "    y = np.array(y) / 255.\n",
        "    \n",
        "    return X, y"
      ],
      "metadata": {
        "id": "4UlZ2WBzGO0D"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Class"
      ],
      "metadata": {
        "id": "Dtqtd611G2p7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ModelClass():\n",
        "\n",
        "  def __init__(self, X_train, X_test, y_train, y_test, loss='binary_crossentropy'):\n",
        "    self.loss=loss\n",
        "    # Split X, y into train and test \n",
        "    self.X_train = X_train\n",
        "    self.X_test = X_test\n",
        "    self.y_train = y_train\n",
        "    self.y_test = y_test\n",
        "    \n",
        "\n",
        "    self.experiment_name = EXPERIMENT_NAME\n",
        "      \n",
        "  def convolution_operation(self, entered_input, filters=64):\n",
        "            \n",
        "      # Taking first input and implementing the first conv block\n",
        "      conv1 = Conv2D(filters, kernel_size = (3,3), padding = \"same\")(entered_input)\n",
        "      batch_norm1 = BatchNormalization()(conv1)\n",
        "      act1 = ReLU()(batch_norm1)\n",
        "\n",
        "      # Taking first input and implementing the second conv block\n",
        "      conv2 = Conv2D(filters, kernel_size = (3,3), padding = \"same\")(act1)\n",
        "      batch_norm2 = BatchNormalization()(conv2)\n",
        "      act2 = ReLU()(batch_norm2)\n",
        "\n",
        "      ### \n",
        "      # padding = 'same' to maintain the same shapes\n",
        "\n",
        "      return act2\n",
        "\n",
        "  def encoder(self, entered_input, filters=64):\n",
        "      # Collect the start and end of each sub-block for normal pass and skip connections\n",
        "      enc1 = self.convolution_operation(entered_input, filters)\n",
        "      MaxPool1 = MaxPooling2D(strides = (2,2))(enc1)\n",
        "      return enc1, MaxPool1\n",
        "\n",
        "  def decoder(self, entered_input, skip, filters=64):\n",
        "      # Upsampling and concatenating the essential features\n",
        "      Upsample = Conv2DTranspose(filters, (2, 2), strides=2, padding=\"same\")(entered_input)\n",
        "      Connect_Skip = Concatenate()([Upsample, skip])\n",
        "      out = self.convolution_operation(Connect_Skip, filters)\n",
        "      return out\n",
        "\n",
        "\n",
        "  def U_Net(self, Image_Size):\n",
        "      # Take the image size and shape\n",
        "      input1 = Input(Image_Size)\n",
        "\n",
        "      # Construct the encoder blocks\n",
        "      skip1, encoder_1 = self.encoder(input1, 64)\n",
        "      skip2, encoder_2 = self.encoder(encoder_1, 64*2)\n",
        "      skip3, encoder_3 = self.encoder(encoder_2, 64*4)\n",
        "      skip4, encoder_4 = self.encoder(encoder_3, 64*8)\n",
        "\n",
        "      # Preparing the next block\n",
        "      conv_block = self.convolution_operation(encoder_4, 64*16)\n",
        "\n",
        "      # Construct the decoder blocks\n",
        "      decoder_1 = self.decoder(conv_block, skip4, 64*8)\n",
        "      decoder_2 = self.decoder(decoder_1, skip3, 64*4)\n",
        "      decoder_3 = self.decoder(decoder_2, skip2, 64*2)\n",
        "      decoder_4 = self.decoder(decoder_3, skip1, 64)\n",
        "\n",
        "      out = Conv2D(1, 1, padding=\"same\", activation=\"sigmoid\")(decoder_4)\n",
        "\n",
        "      model = Model(input1, out)\n",
        "      return model\n",
        "\n",
        "  def set_model(self):\n",
        "      # Instantiate Model\n",
        "      input_shape = self.X_train.shape[1:]\n",
        "      self.model = self.U_Net(input_shape)\n",
        "      \n",
        "      # Compile Model\n",
        "      self.model.compile(loss=self.loss, \n",
        "                    optimizer='adam')\n",
        "\n",
        "  def run(self):\n",
        "\n",
        "      print(80*'-')\n",
        "      print('------MODEL RUNNING------')\n",
        "\n",
        "      # set model\n",
        "      self.set_model()\n",
        "\n",
        "      mc = ModelCheckpoint('oxford_segmentation.h5', save_best_only=True)\n",
        "      \n",
        "      self.model.fit(self.X_train, self.y_train, validation_split=0.3,\n",
        "                      batch_size=BATCH_SIZE, epochs=EPOCHS, callbacks=[mc])\n",
        "      \n",
        "      self.mlflow_log_param('loss', self.loss)\n",
        "\n",
        "      print(80*'=')\n",
        "      print('------MODEL SUCCESFULLY------')\n",
        "      \n",
        "\n",
        "  def evaluate(self):\n",
        "      print(80*'-')\n",
        "      print('------MODEL EVALUATING------')        \n",
        "      results = self.model.evaluate(self.X_test, self.y_test)\n",
        "      self.mlflow_log_metric('loss', results)\n",
        "      print(80*'=')\n",
        "      print('------MODEL EVALUATED------')\n",
        "      \n",
        "      \n",
        "  @memoized_property\n",
        "  def mlflow_client(self):\n",
        "      mlflow.set_tracking_uri(MLFLOW_URI)\n",
        "      return MlflowClient()\n",
        "\n",
        "  @memoized_property\n",
        "  def mlflow_experiment_id(self):\n",
        "      try:\n",
        "          return self.mlflow_client.create_experiment(self.experiment_name)\n",
        "      except BaseException:\n",
        "          return self.mlflow_client.get_experiment_by_name(self.experiment_name).experiment_id\n",
        "\n",
        "  @memoized_property\n",
        "  def mlflow_run(self):\n",
        "      return self.mlflow_client.create_run(self.mlflow_experiment_id)\n",
        "\n",
        "  def mlflow_log_param(self, key, value):\n",
        "      self.mlflow_client.log_param(self.mlflow_run.info.run_id, key, value)\n",
        "\n",
        "  def mlflow_log_metric(self, key, value):\n",
        "      self.mlflow_client.log_metric(self.mlflow_run.info.run_id, key, value)\n",
        "          "
      ],
      "metadata": {
        "id": "TDgb2OLdGOyA"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing code"
      ],
      "metadata": {
        "id": "O8FsLVo_Hfay"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get data\n",
        "X, y = get_data()\n",
        "\n"
      ],
      "metadata": {
        "id": "aGRRlZg6GOre"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s6Hg9PyIKaym",
        "outputId": "bce71b78-f703-44ef-a1a6-1d81f4463b1b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(15, 160, 160, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)"
      ],
      "metadata": {
        "id": "LUVw3nrwLW2o"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# instantiate model \n",
        "model = ModelClass(X_train, X_test, y_train, y_test)\n"
      ],
      "metadata": {
        "id": "ZV5Q2NvxGOpH"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.run()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lRnZlu6lGOmx",
        "outputId": "d06746d6-54ce-4dcc-be64-7218bfb48dc2"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "------MODEL RUNNING------\n",
            "Epoch 1/2\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.9066 - val_loss: 0.7357\n",
            "Epoch 2/2\n",
            "1/1 [==============================] - 0s 246ms/step - loss: 0.7988 - val_loss: 0.7371\n",
            "================================================================================\n",
            "------MODEL SUCCESFULLY------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NZMSclE6GOj5",
        "outputId": "b727e44f-dad5-4072-ce0f-e154441bba3c"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "------MODEL EVALUATING------\n",
            "1/1 [==============================] - 0s 96ms/step - loss: 0.7293\n",
            "================================================================================\n",
            "------MODEL EVALUATED------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "6MeOnle3GOhk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "9zVXeN6VGOet"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}