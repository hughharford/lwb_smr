{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea87dd9-bcea-4a53-a6a5-c50122d79c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST RUN, VERY LOW BATCH AND DATA SIZES > SHOWS 2 SE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f260c50-8b65-4279-9362-e345f624d1ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MLFLOW_URI = \"https://mlflow.lewagon.ai/\"\n",
    "EXPERIMENT_NAME = \"UK Lon lwb_smr vertex_run_02\" # template\n",
    "EXPERIMENT_TAGS = {\n",
    "    'USER': 'hsth',\n",
    "    'RUN NAME': 'test_vertex2, operational',\n",
    "    'VERSION': 'M2_R04_15',\n",
    "    'DESCRIPTION': 'test_Model VGG16 UNet, 20 epochs, 72k images',\n",
    "    'LOSS': 'dice',\n",
    "    'METRICS': 'accuracy, binaryIoU, AUC'\n",
    "}\n",
    "\n",
    "UNET_INPUT_SHAPE = (224,224,3)\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 2\n",
    "# LOSS='binary_crossentropy'\n",
    "LOSS = 'DICE'\n",
    "TEST_CHECKPOINT_PATH_N_FILE_NAME = \"\"\n",
    "TEST_MODEL_PATH_N_FILE_NAME = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2d074b-ef96-4cd9-a984-d8ae94a895e9",
   "metadata": {},
   "source": [
    "# TENSOR SLICE DATA LOADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c942a20-9b3f-4303-9a83-4e141510352f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf600518-8e02-4241-b689-c1e4ebb6a386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoints\t\tmodels\t\t       vertex_data_jpegs.zip\n",
      "image_datasets_csv\ttrain_RGB_tiles_jpeg\n",
      "image_datasets_csv.zip\ttrain_mask_tiles_jpeg\n"
     ]
    }
   ],
   "source": [
    "!ls '../../../raw_data/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f1bfa94-84a3-4fcf-9cb1-dfed162302d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "x_path = '../../../raw_data/train_RGB_tiles_jpeg/'\n",
    "x_images = os.listdir(x_path)\n",
    "y_path = '../../../raw_data/train_mask_tiles_jpeg/'\n",
    "y_masks = os.listdir(y_path)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b1e7035-69de-469b-ae03-ee3a58096be1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../../../raw_data/train_RGB_tiles_jpeg',\n",
       " '../../../raw_data/train_mask_tiles_jpeg']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root_path = '../../../raw_data/'\n",
    "folders = ['train_RGB_tiles_jpeg', 'train_mask_tiles_jpeg']\n",
    "folder_path = [f'{root_path}{folder}' for folder in folders]\n",
    "folder_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4bb5bde4-64e3-4cc3-8319-c1e2efcce390",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images, train_mask = [], []\n",
    "# train_images =[f'../../../raw_data/train_RGB_tiles_jpeg/{filename}' for filename in os.listdir(folder_path[0])]\n",
    "for i, filename in enumerate(os.listdir(folder_path[0])):\n",
    "    if i == 2*BATCH_SIZE: break\n",
    "    train_images.append(f'../../../raw_data/train_RGB_tiles_jpeg/{filename}')\n",
    "    \n",
    "# train_mask = [f'../../../raw_data/train_mask_tiles_jpeg/{filename}' for filename in os.listdir(folder_path[1])]\n",
    "for i, filename in enumerate(os.listdir(folder_path[1])):\n",
    "    if i == 2*BATCH_SIZE: break\n",
    "    train_mask.append(f'../../../raw_data/train_mask_tiles_jpeg/{filename}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95e5a4c1-57e8-4dad-90f4-58f9ec6edcef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16,\n",
       "                                           image_path  \\\n",
       " 0  ../../../raw_data/train_RGB_tiles_jpeg/austin1...   \n",
       " 1  ../../../raw_data/train_RGB_tiles_jpeg/austin2...   \n",
       " 2  ../../../raw_data/train_RGB_tiles_jpeg/austin6...   \n",
       " 3  ../../../raw_data/train_RGB_tiles_jpeg/austin7...   \n",
       " 4  ../../../raw_data/train_RGB_tiles_jpeg/chicago...   \n",
       " \n",
       "                                            mask_path  \n",
       " 0  ../../../raw_data/train_mask_tiles_jpeg/austin...  \n",
       " 1  ../../../raw_data/train_mask_tiles_jpeg/austin...  \n",
       " 2  ../../../raw_data/train_mask_tiles_jpeg/chicag...  \n",
       " 3  ../../../raw_data/train_mask_tiles_jpeg/chicag...  \n",
       " 4  ../../../raw_data/train_mask_tiles_jpeg/kitsap...  )"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images.sort()\n",
    "train_mask.sort()\n",
    "\n",
    "\n",
    "train_df = pd.DataFrame()\n",
    "# train_df['file_path'] = train_images\n",
    "train_df['image_path'] = train_images\n",
    "train_df['mask_path'] = train_mask\n",
    "\n",
    "len(train_df), train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20b3769d-24d9-4378-97e2-28ae1ee6e243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef7e9acb-ef27-49fe-b037-93e9ae4e6990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# root_path = '/raw_data/'\n",
    "\n",
    "# # raw_data/train_mask_tiles_jpeg\n",
    "\n",
    "# x_path = root_path + 'train_RGB_tiles_jpeg/'\n",
    "# x_images = os.listdir(x_path)\n",
    "# y_path = root_path + 'train_mask_tiles_jpeg/'\n",
    "# y_masks = os.listdir(y_path)\n",
    "    \n",
    "\n",
    "# folders = ['train_RGB_tiles_jpeg', 'train_mask_tiles_jpeg']\n",
    "# folder_path = [f'{root_path}{folder}' for folder in folders]\n",
    "# folder_path\n",
    "\n",
    "# train_images, train_mask = [], []\n",
    "# train_images = [f'{x_images}{filename}' for filename in os.listdir(folder_path[0])]\n",
    "# train_mask = [f'{y_masks}{filename}' for filename in os.listdir(folder_path[1])]\n",
    "\n",
    "# train_images.sort()\n",
    "# train_mask.sort()\n",
    "\n",
    "\n",
    "# train_df = pd.DataFrame()\n",
    "# # train_df['file_path'] = train_images\n",
    "# train_df['image_path'] = train_images\n",
    "# train_df['mask_path'] = train_mask\n",
    "\n",
    "# train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "237149c5-1be0-4939-a96e-b7709a28fef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-12 04:54:57.454391: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-12 04:54:57.471360: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-12 04:54:57.471697: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-12 04:54:57.473218: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-12 04:54:57.474654: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-12 04:54:57.475004: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-12 04:54:57.475302: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-12 04:54:58.004933: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-12 04:54:58.005330: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-12 04:54:58.005610: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-12 04:54:58.005866: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def holdout(df, train_ratio=0.8, test_to_val_ratio=0.5, include_all=False):\n",
    "\n",
    "    img_paths = df[\"image_path\"].values\n",
    "    msk_paths = df[\"mask_path\"].values\n",
    "\n",
    "    df_mask = df.copy()\n",
    "\n",
    "    df_train, df_val = train_test_split(df_mask, train_size=train_ratio)\n",
    "    df_test, df_val = train_test_split(df_val, test_size=test_to_val_ratio)\n",
    "\n",
    "    ds_train = tf.data.Dataset.from_tensor_slices(\n",
    "         (df_train[\"image_path\"].values, df_train[\"mask_path\"].values)\n",
    "    )\n",
    "    ds_val = tf.data.Dataset.from_tensor_slices(\n",
    "        (df_val[\"image_path\"].values, df_val[\"mask_path\"].values)\n",
    "    )\n",
    "    ds_test = tf.data.Dataset.from_tensor_slices(\n",
    "        (df_test[\"image_path\"].values, df_test[\"mask_path\"].values)\n",
    "    )\n",
    "\n",
    "    return ds_train, ds_val, ds_test\n",
    "\n",
    "ds_train, ds_val, ds_test = holdout(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "052c664a-fcc9-4d0d-b32b-354c71c72f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_path(input_path, mask_path):\n",
    "    \"\"\"\n",
    "    Load images from files.\n",
    "    :input_path: the path to the satellite file\n",
    "    :mask_path: the path to the mask file\n",
    "    :return: The image and mask\n",
    "    .. note:: Works with jpg images \n",
    "              Only the first channel is kept for the mask\n",
    "    \"\"\"\n",
    "    \n",
    "    IMAGE_SQ_SIZE = 224\n",
    "\n",
    "    input_img = tf.io.read_file(input_path)   \n",
    "    input_img = tf.io.decode_jpeg(input_img, channels=3)\n",
    "    input_img =  tf.image.resize(input_img, [IMAGE_SQ_SIZE, IMAGE_SQ_SIZE])\n",
    "\n",
    "    mask_img = tf.io.read_file(mask_path)   \n",
    "    mask_img = tf.io.decode_jpeg(mask_img, channels=1)\n",
    "    mask_img =  tf.image.resize(mask_img, [IMAGE_SQ_SIZE, IMAGE_SQ_SIZE])\n",
    "\n",
    "   \n",
    "    return input_img, mask_img\n",
    "\n",
    "def normalize(image, mask):\n",
    "    # image = tf.cast(image, tf.float32) / 255.\n",
    "\n",
    "    return tf.math.divide(image, 255), tf.math.divide(mask, 255) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "480522e8-7f1d-427b-a957-19adf478be05",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "ds_train = ds_train.map(process_path) \\\n",
    ".map(normalize) \\\n",
    ".batch(batch_size=BATCH_SIZE) \\\n",
    ".prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "ds_val = ds_val.map(process_path) \\\n",
    ".map(normalize) \\\n",
    ".batch(batch_size=BATCH_SIZE) \\\n",
    ".prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7c093c-c72a-4859-b6c4-b57dd9ba9cf9",
   "metadata": {},
   "source": [
    "### we do not use:\n",
    "# CustomDataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "54a2e5ae-7cc4-4c46-b5bc-04375ac789e2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from tensorflow import keras\n",
    "# import tensorflow as tf\n",
    "# import numpy as np\n",
    "# import math\n",
    "# import os\n",
    "# from skimage.transform import resize\n",
    "# from PIL import Image\n",
    "\n",
    "# class CustomDataLoader(keras.utils.Sequence):\n",
    "#     \"\"\" Allow custom data import and output from image files for masking \"\"\"\n",
    "\n",
    "#     def __init__(self, x_images, x_path, y_masks, y_path, input_image_size, batch_size):\n",
    "#         \"\"\"\n",
    "#         x_images            is a list of RGB images in a directory\n",
    "#         x_path              is the path for the images\n",
    "#         y_masks             is a list of greyscale masks in a directory\n",
    "#         y_path              is the path for the image masks\n",
    "#         input_image_size    tuple of image size e.g. (250,250)\n",
    "#         batch_size   size of the batches e.g. 16, 32 etc.\n",
    "#         \"\"\"\n",
    "#         self.x, self.y = x_images, y_masks\n",
    "#         self.x_path, self.y_path = x_path, y_path\n",
    "#         self.input_image_size = input_image_size\n",
    "#         self.batch_size = batch_size\n",
    "\n",
    "#     def __len__(self):\n",
    "#         \"\"\"\n",
    "#         return the number of batches required for the amount of\n",
    "#         images in x_images\n",
    "#         \"\"\"\n",
    "#         return math.ceil(len(self.x) / self.batch_size)\n",
    "\n",
    "#     def __getitem__(self,batch_index_position):\n",
    "#         # batch_index_position denotes start of current batch\n",
    "#         # current_index determines end index of current batch\n",
    "#         current_index = batch_index_position * self.batch_size\n",
    "\n",
    "#         # create lists of the x and y image names in the current batch\n",
    "#         batch_x = self.x[current_index:current_index+self.batch_size]\n",
    "#         batch_y = self.y[current_index:current_index+self.batch_size]\n",
    "\n",
    "#         # UNET_INPUT_SHAPE = (224,224,3)\n",
    "\n",
    "#         self.resized_size = UNET_INPUT_SHAPE\n",
    "\n",
    "#         # create a list for the tensorflow objects then read in images\n",
    "#         # and convert to tensorflow objects.\n",
    "#         xl = []\n",
    "        \n",
    "#         for i, ximg in enumerate(batch_x):\n",
    "#             img = tf.io.read_file(self.x_path+batch_x[i])\n",
    "#             img = tf.io.decode_jpeg(img, channels=3)\n",
    "#             # resizing\n",
    "#             input_img =  tf.image.resize(img, [self.resized_size[0], self.resized_size[1]])\n",
    "#             # normalise\n",
    "#             input_img = tf.math.divide(input_img, 255)\n",
    "#             xl.append(input_img)\n",
    "#         x = tf.stack(xl)\n",
    "\n",
    "#         # create a list for the tensorflow objects then read in images\n",
    "#         # and convert to tensorflow objects.\n",
    "#         # mask images are single channel\n",
    "#         yl = []\n",
    "#         for j, yimg in enumerate(batch_y):\n",
    "#             mask = tf.io.read_file(self.y_path+batch_y[i])\n",
    "#             mask = tf.io.decode_jpeg(mask, channels=1)\n",
    "#             # resizing\n",
    "#             mask_img =  tf.image.resize(mask, [self.resized_size[0], self.resized_size[1]])\n",
    "#             # normalise\n",
    "#             mask_img = tf.math.divide(mask_img, 255)\n",
    "#             yl.append(mask_img)\n",
    "#         y = tf.stack(yl)\n",
    "\n",
    "#         return x,y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7d74db3d-59f2-48b4-b631-0a60de6bfa49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d09839d0-ec2f-465b-bf22-0535f6e5cdae",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import os\n",
    "\n",
    "# class GetData():\n",
    "#     '''\n",
    "#     Custom function to generate train, validation and optional test datasets\n",
    "#     by specifying percentages. Saves to a csv file for reading in and out.\n",
    "#     '''\n",
    "\n",
    "#     def __init__(self,train_path,test_path,train_pc,val_pc,test_pc = 0.0):\n",
    "#         '''\n",
    "#         Specify train and ground truth (\"test\") paths\n",
    "#         train_pc = percentage of images for training\n",
    "#         val_pc   = percentage of images for validation purposes\n",
    "#         test_pc  = [optional] reserve percentage of images for testing\n",
    "#                    default is 0.0%\n",
    "#         '''\n",
    "#         self.train_path = train_path\n",
    "#         self.test_path  = test_path\n",
    "#         # Percentages\n",
    "#         self.train_pc = 1.0 - (val_pc + test_pc)\n",
    "#         self.val_pc = 1.0 - (train_pc + test_pc)\n",
    "#         self.test_pc = test_pc\n",
    "\n",
    "#     def make_dataframe(self):\n",
    "#         '''\n",
    "#         Create a dataframe of all available images in the train and test paths\n",
    "#         '''\n",
    "#         train_list = os.listdir(self.train_path)\n",
    "#         train_list.sort()\n",
    "\n",
    "#         test_list  = os.listdir(self.test_path)\n",
    "#         test_list.sort()\n",
    "\n",
    "#         self.data_df = pd.DataFrame(list(zip(train_list,test_list)), columns=['x_data','y_data'])\n",
    "#         return self.data_df\n",
    "\n",
    "#     def check_data(self,df,check_set):\n",
    "#         '''\n",
    "#         Check that the x and y data matches\n",
    "#         '''\n",
    "#         self.check = 0 # flag to allow later steps to proceed if datasets match\n",
    "\n",
    "#         validate = (df.x_data == df.y_data.str.replace(\"_mask\",\"\")).sum()\n",
    "#         if validate == len(df):\n",
    "#             self.check = 1\n",
    "#             return self.check\n",
    "#         else:\n",
    "#             return self.check\n",
    "\n",
    "#     def data_split(self):\n",
    "#         '''\n",
    "#         Split the dataset according to the pre-defined percentages\n",
    "#         Logic curated in specific way to ensure that valid int values are used\n",
    "#         '''\n",
    "#         n_train = int(len(self.data_df) * self.train_pc)\n",
    "#         # to insure equal splits of integer values, validation is checked against\n",
    "#         # the test set if it exists\n",
    "#         if self.test_pc > 0.0:\n",
    "#             n_val   = int(len(self.data_df) * self.val_pc)\n",
    "#             n_test  = len(self.data_df) - n_train - n_val\n",
    "#         # if no test set specified then n_val is just the total length minus train set\n",
    "#         else:\n",
    "#             n_val   = len(self.data_df) - n_train\n",
    "\n",
    "#         # create a dataframe to pull out lists without replacement\n",
    "#         data_split = self.data_df.copy()\n",
    "\n",
    "#         # create the train df and then remove the train rows:\n",
    "#         self.data_train_df = data_split.sample(n=n_train,random_state=42)\n",
    "#         data_split.drop(data_split.index[[tuple(self.data_train_df.index)]])\n",
    "\n",
    "#         # create the validation df and then remove the val rows\n",
    "#         # still checks to see if a test set is being requested\n",
    "#         if self.test_pc > 0.0:\n",
    "#             self.data_val_df = data_split.sample(n=n_val,random_state=42)\n",
    "#             data_split.drop(data_split.index[[tuple(self.data_val_df.index)]])\n",
    "#             # test df takes what is remaining\n",
    "#             self.data_test_df = data_split.copy()\n",
    "#         else:\n",
    "#             self.data_val_df = data_split.copy()\n",
    "\n",
    "#     def get_datasets(self):\n",
    "#         '''\n",
    "#         Create the dataframes and convert to dictory with lists\n",
    "#         additionally perform checks on the data before proceeding\n",
    "#         '''\n",
    "#         self.make_dataframe()\n",
    "#         self.data_split()\n",
    "#         self.data_dict = {'train_x':[],\n",
    "#                           'train_y':[],\n",
    "#                           'val_x':[],\n",
    "#                           'val_y':[],\n",
    "#                           'test_x':[],\n",
    "#                           'test_y':[]}\n",
    "\n",
    "#         compiled_checks = 0\n",
    "#         ### CHECKS ###\n",
    "#         if self.check_data(self.data_train_df,\"Training Data\") == 0:\n",
    "#             return print(\"Training Data: ERROR ***UNMATCHED*** datasets, please go back and check input paths and image directories\")\n",
    "#         compiled_checks += 1\n",
    "\n",
    "#         if self.check_data(self.data_val_df,  \"Validation Data\") == 0:\n",
    "#             return print(\"Training Data: ERROR ***UNMATCHED*** datasets, please go back and check input paths and image directories\")\n",
    "#         compiled_checks += 1\n",
    "\n",
    "#         if self.test_pc > 0.0:\n",
    "#             if self.check_data(self.data_test_df, \"Testing Data\") == 0:\n",
    "#                 return print(\"Test Data: ERROR ***UNMATCHED*** datasets, please go back and check input paths and image directories\")\n",
    "#             compiled_checks += 1\n",
    "\n",
    "#         if self.val_pc > 0.0:\n",
    "#             dict = self.make_dict(with_val = 1)\n",
    "#         else:\n",
    "#             dict = self.make_dict(with_val = 0)\n",
    "\n",
    "#         return dict\n",
    "\n",
    "#     def make_dict(self,with_val):\n",
    "#         '''\n",
    "#         make dictionary of the datasets\n",
    "#         '''\n",
    "#         if with_val == 1:\n",
    "#             print(\"Datasets match, proceed\")\n",
    "#             self.data_dict['train_x'] = list(self.data_train_df.x_data)\n",
    "#             self.data_dict['train_y'] = list(self.data_train_df.y_data)\n",
    "#             self.data_dict['val_x'] = list(self.data_val_df.x_data)\n",
    "#             self.data_dict['val_y'] = list(self.data_val_df.y_data)\n",
    "#             self.data_dict['test_x'] = list(self.data_test_df.x_data)\n",
    "#             self.data_dict['test_y'] = list(self.data_test_df.y_data)\n",
    "\n",
    "#         elif with_val == 0:\n",
    "#             print(\"Datasets match, proceed\")\n",
    "#             self.data_dict['train_x'] = list(self.data_train_df.x_data)\n",
    "#             self.data_dict['train_y'] = list(self.data_train_df.y_data)\n",
    "#             self.data_dict['val_x'] = list(self.data_val_df.x_data)\n",
    "#             self.data_dict['val_y'] = list(self.data_val_df.y_data)\n",
    "\n",
    "#         return self.data_dict\n",
    "\n",
    "#     def save_datasets(self, save_path):\n",
    "#         '''\n",
    "#         allows for saving of dataframes so can consistently use the same datasets\n",
    "#         Specifiy save path that is DIFFERENT from the images paths\n",
    "#         '''\n",
    "#         self.data_train_df.to_csv(save_path+\"train_dataset.csv\")\n",
    "#         self.data_val_df.to_csv(save_path+\"validation_dataset.csv\")\n",
    "#         if self.test_pc > 0.0:\n",
    "#             self.data_test_df.to_csv(save_path+\"test_dataset.csv\")\n",
    "\n",
    "# class LoadDataSets(GetData):\n",
    "#     '''\n",
    "#     Load the pre-saved lists of shuffled images back in and\n",
    "#     produce the list dictionary\n",
    "#     '''\n",
    "\n",
    "#     def __init__(self, load_train, load_val, load_test = \"none\"):\n",
    "#         '''\n",
    "#         allows for loading of already generated datasets\n",
    "#         input full filepath for each dataset to load\n",
    "#         load_train  - Full filepath and filename of training .csv file\n",
    "#         load_val    - Full filepath and filename of validation .csv file\n",
    "#         load_test   - Full filepath and filename of test .csv file\n",
    "#         '''\n",
    "#         self.load_train = load_train\n",
    "#         self.load_val = load_val\n",
    "#         self.load_test = load_test\n",
    "\n",
    "#     def load_datasets(self):\n",
    "#         self.data_dict = {'train_x':[],\n",
    "#                       'train_y':[],\n",
    "#                       'val_x':[],\n",
    "#                       'val_y':[],\n",
    "#                       'test_x':[],\n",
    "#                       'test_y':[]}\n",
    "#         self.data_train_df = pd.read_csv(self.load_train, index_col = 0)\n",
    "#         self.data_val_df   = pd.read_csv(self.load_val, index_col = 0)\n",
    "\n",
    "\n",
    "#         if self.load_test != \"none\":\n",
    "#             self.data_test_df = pd.read_csv(self.load_test, index_col = 0)\n",
    "#             dict = self.make_dict(with_val = 1)\n",
    "#         else:\n",
    "#             dict = self.make_dict(with_val = 0)\n",
    "\n",
    "#         return dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7925488-670a-4d53-97e9-fde6788e6c0d",
   "metadata": {},
   "source": [
    "# model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1ab6d800-39ea-4458-bdcb-abff179bd1ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Activation, ReLU\n",
    "from tensorflow.keras.layers import BatchNormalization, Conv2DTranspose, Concatenate\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "from tensorflow.keras.applications import VGG16\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class SMR_Model():\n",
    "    ''' creating our first lwb_smr models '''\n",
    "\n",
    "    def __init__(self, input_shape):\n",
    "        self.input_shape = input_shape\n",
    "\n",
    "    def get_latest_model(self):\n",
    "        model = self.build_vgg16_unet(self.input_shape)\n",
    "        model = self.compile_model(model)\n",
    "\n",
    "        return model\n",
    "\n",
    "    def convolution_block(self, inputs, num_filters):\n",
    "        ''' simple UNET convolution block with BatchNormalisation '''\n",
    "\n",
    "        # convolution layer 1 of the block\n",
    "        x = Conv2D(num_filters, (3,3), padding='same')(inputs)  # padding='same' to avoid cut-down with conv\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('relu')(x)\n",
    "\n",
    "        # convolution layer 2 of the block\n",
    "        x = Conv2D(num_filters, (3,3), padding='same')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('relu')(x)\n",
    "\n",
    "        # max pooling not used here as just the bridge\n",
    "\n",
    "        return x\n",
    "\n",
    "    def decoder_block(self, inputs, skip_tensor, num_filters):\n",
    "        ''' decoder block for UNET '''\n",
    "        # adds in the skips with concatenate\n",
    "        x = Conv2DTranspose(num_filters, (2,2), strides=2, padding='same')(inputs) # stride important here to up-sample\n",
    "        x = Concatenate()([x, skip_tensor])     # bringing in skip layer\n",
    "        x = self.convolution_block(x, num_filters)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def build_vgg16_unet(self, input_shape):\n",
    "        ''' build vgg-16 '''\n",
    "\n",
    "        inputs = Input(input_shape)\n",
    "\n",
    "        # see actual VGG-16 here: https://github.com/keras-team/keras/blob/v2.9.0/keras/applications/vgg16.py#L43-L227\n",
    "        vgg16 = VGG16(include_top=False, weights='imagenet', input_tensor=inputs)\n",
    "        # vgg16.summary()\n",
    "        vgg16.trainable = False\n",
    "\n",
    "        ''' Encoder - skip layers '''\n",
    "        skip1 = vgg16.get_layer('block1_conv2').output #  256 x 256, 64 filters in vgg16\n",
    "        skip2 = vgg16.get_layer('block2_conv2').output #  128 x 128, 128 filters in vgg16\n",
    "        skip3 = vgg16.get_layer('block3_conv3').output #   64 x 64, 256 filters in vgg16\n",
    "        skip4 = vgg16.get_layer('block4_conv3').output #   32 x 32, 512 filters in vgg16\n",
    "        # display('skip4: ' + str(skip4.shape))\n",
    "\n",
    "        # only need to specify the skip layers, as VGG16 is an Encoder\n",
    "        # Therefore, VGG16 comes built with MaxPool2d, so we don't specify\n",
    "\n",
    "        ''' Bridge '''\n",
    "        bridge = vgg16.get_layer('block5_conv3').output # 16 x 16, with 512 filters in vgg16\n",
    "        # display('bridge: ' + str(bridge.shape))\n",
    "\n",
    "\n",
    "        ''' Decoder '''\n",
    "        d1 = self.decoder_block(bridge, skip4, 512) #  512 filters, as per the bridge\n",
    "        d2 = self.decoder_block(d1, skip3, 256) #  256 filters\n",
    "        d3 = self.decoder_block(d2, skip2, 128) #  128 filters\n",
    "        d4 = self.decoder_block(d3, skip1, 64)  #   64 filters\n",
    "\n",
    "        ''' Output '''\n",
    "        outputs = Conv2D(1, (1,1), padding='same', activation='sigmoid')(d4)\n",
    "\n",
    "        model = Model(inputs, outputs, name='first_VGG16_UNET')\n",
    "\n",
    "        return model\n",
    "    \n",
    "    def dice_loss(self, y_true, y_pred):\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        y_pred = tf.math.sigmoid(y_pred)\n",
    "        numerator = 2 * tf.reduce_sum(y_true * y_pred)\n",
    "        denominator = tf.reduce_sum(y_true + y_pred)\n",
    "\n",
    "        return 1 - numerator / denominator\n",
    "\n",
    "\n",
    "    def compile_model(self, m):\n",
    "        ''' with accuracy, binaryIoU, AuC '''\n",
    "        # metrics\n",
    "        threshold = 0.5\n",
    "        binaryIoU = tf.keras.metrics.BinaryIoU(target_class_ids=[1], threshold=threshold)\n",
    "        AuC = tf.keras.metrics.AUC()\n",
    "\n",
    "        # loss\n",
    "        #self.dice_loss = ...\n",
    "        \n",
    "        # Compile Model\n",
    "        m.compile(\n",
    "                    loss=self.dice_loss,\n",
    "                    optimizer='adam',\n",
    "                    metrics=['accuracy', binaryIoU, AuC]\n",
    "                    )\n",
    "        return m\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e02df14-1730-4a67-84d9-6df7918d7cfe",
   "metadata": {
    "tags": []
   },
   "source": [
    "# utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c9f96beb-091b-4ebf-815a-26b02d7ea818",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# mlflow\n",
    "#\n",
    "# and others...\n",
    "\n",
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "from memoized_property import memoized_property\n",
    "\n",
    "MLFLOW_URI = \"https://mlflow.lewagon.ai/\"\n",
    "\n",
    "class PushMLFlow():\n",
    "    '''\n",
    "        MLFLOW_URI = \"https://mlflow.lewagon.ai/\"\n",
    "        EXPERIMENT_NAME = \"[UK] [LONDON] [SOLAR_ROOF] TEST RUN\" # template\n",
    "        EXPERIMENT_TAGS = {\n",
    "            'USER': 'test_user',\n",
    "            'RUN NAME': 'test runs',\n",
    "            'VERSION': '1.0.1',\n",
    "            'LOSS': 'dice'\n",
    "            'DESCRIPTION': 'testing MLFlow Pipeline. Model - basic U-Net structure, 2 epochs, 15 images'\n",
    "        }\n",
    "    '''\n",
    "\n",
    "    def __init__(self, experiment_name, experiment_tags):\n",
    "        self.experiment_name = experiment_name\n",
    "        self.experiment_tag = experiment_tags\n",
    "\n",
    "    @memoized_property\n",
    "    def mlflow_client(self):\n",
    "        mlflow.set_tracking_uri(MLFLOW_URI)\n",
    "        return MlflowClient()\n",
    "\n",
    "    @memoized_property\n",
    "    def mlflow_experiment_id(self):\n",
    "        try:\n",
    "            return self.mlflow_client.create_experiment(self.experiment_name)\n",
    "        except BaseException:\n",
    "            return self.mlflow_client.get_experiment_by_name(self.experiment_name).experiment_id\n",
    "\n",
    "    @memoized_property\n",
    "    def mlflow_run(self):\n",
    "        return self.mlflow_client.create_run(self.mlflow_experiment_id, tags=self.experiment_tag)\n",
    "\n",
    "    def mlflow_log_param(self, key, value):\n",
    "        self.mlflow_client.log_param(self.mlflow_run.info.run_id, key, value)\n",
    "\n",
    "    def mlflow_log_metric(self, key, value):\n",
    "        self.mlflow_client.log_metric(self.mlflow_run.info.run_id, key, value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c419dfba-a83b-4328-85d2-da6fef000627",
   "metadata": {},
   "source": [
    "# trainer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6352a612-b695-41e5-83a0-828ecdd13dc8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "------SETTING FOR DATA RUN------\n",
      "--------------------------------------------------------------------------------\n",
      "------MODEL RUNNING------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-12 04:55:08.619697: W tensorflow/core/common_runtime/bfc_allocator.cc:462] Allocator (GPU_0_bfc) ran out of memory trying to allocate 576.0KiB (rounded to 589824)requested by op Mul\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2022-06-12 04:55:08.619746: I tensorflow/core/common_runtime/bfc_allocator.cc:1010] BFCAllocator dump for GPU_0_bfc\n",
      "2022-06-12 04:55:08.619760: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (256): \tTotal Chunks: 15, Chunks in use: 15. 3.8KiB allocated for chunks. 3.8KiB in use in bin. 569B client-requested in use in bin.\n",
      "2022-06-12 04:55:08.619767: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (512): \tTotal Chunks: 1, Chunks in use: 1. 512B allocated for chunks. 512B in use in bin. 512B client-requested in use in bin.\n",
      "2022-06-12 04:55:08.619774: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (1024): \tTotal Chunks: 1, Chunks in use: 1. 1.2KiB allocated for chunks. 1.2KiB in use in bin. 1.0KiB client-requested in use in bin.\n",
      "2022-06-12 04:55:08.619781: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (2048): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-06-12 04:55:08.619787: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (4096): \tTotal Chunks: 1, Chunks in use: 1. 6.8KiB allocated for chunks. 6.8KiB in use in bin. 6.8KiB client-requested in use in bin.\n",
      "2022-06-12 04:55:08.619794: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (8192): \tTotal Chunks: 1, Chunks in use: 0. 10.8KiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-06-12 04:55:08.619800: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (16384): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-06-12 04:55:08.619806: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (32768): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-06-12 04:55:08.619823: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (65536): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-06-12 04:55:08.619831: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (131072): \tTotal Chunks: 1, Chunks in use: 1. 144.0KiB allocated for chunks. 144.0KiB in use in bin. 144.0KiB client-requested in use in bin.\n",
      "2022-06-12 04:55:08.619837: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (262144): \tTotal Chunks: 3, Chunks in use: 1. 864.0KiB allocated for chunks. 288.0KiB in use in bin. 288.0KiB client-requested in use in bin.\n",
      "2022-06-12 04:55:08.619844: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (524288): \tTotal Chunks: 1, Chunks in use: 1. 761.0KiB allocated for chunks. 761.0KiB in use in bin. 576.0KiB client-requested in use in bin.\n",
      "2022-06-12 04:55:08.619850: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (1048576): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-06-12 04:55:08.619855: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (2097152): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-06-12 04:55:08.619879: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (4194304): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-06-12 04:55:08.619884: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (8388608): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-06-12 04:55:08.619890: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (16777216): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-06-12 04:55:08.619896: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (33554432): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-06-12 04:55:08.619902: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (67108864): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-06-12 04:55:08.619908: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (134217728): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-06-12 04:55:08.619918: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (268435456): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-06-12 04:55:08.619926: I tensorflow/core/common_runtime/bfc_allocator.cc:1033] Bin for 576.0KiB was 512.0KiB, Chunk State: \n",
      "2022-06-12 04:55:08.619931: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] Next region of size 1835008\n",
      "2022-06-12 04:55:08.619943: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f096ae00000 of size 1280 next 1\n",
      "2022-06-12 04:55:08.619949: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f096ae00500 of size 256 next 2\n",
      "2022-06-12 04:55:08.619954: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f096ae00600 of size 256 next 3\n",
      "2022-06-12 04:55:08.619960: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f096ae00700 of size 256 next 4\n",
      "2022-06-12 04:55:08.619965: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f096ae00800 of size 256 next 5\n",
      "2022-06-12 04:55:08.619970: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f096ae00900 of size 256 next 6\n",
      "2022-06-12 04:55:08.619975: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f096ae00a00 of size 256 next 7\n",
      "2022-06-12 04:55:08.619981: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f096ae00b00 of size 256 next 8\n",
      "2022-06-12 04:55:08.619986: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f096ae00c00 of size 256 next 11\n",
      "2022-06-12 04:55:08.619991: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f096ae00d00 of size 256 next 12\n",
      "2022-06-12 04:55:08.619996: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f096ae00e00 of size 256 next 13\n",
      "2022-06-12 04:55:08.620001: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f096ae00f00 of size 256 next 16\n",
      "2022-06-12 04:55:08.620006: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f096ae01000 of size 256 next 17\n",
      "2022-06-12 04:55:08.620012: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f096ae01100 of size 512 next 20\n",
      "2022-06-12 04:55:08.620017: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f096ae01300 of size 256 next 21\n",
      "2022-06-12 04:55:08.620022: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f096ae01400 of size 256 next 22\n",
      "2022-06-12 04:55:08.620028: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f096ae01500 of size 256 next 23\n",
      "2022-06-12 04:55:08.620033: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7f096ae01600 of size 11008 next 9\n",
      "2022-06-12 04:55:08.620038: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f096ae04100 of size 6912 next 10\n",
      "2022-06-12 04:55:08.620043: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7f096ae05c00 of size 294912 next 15\n",
      "2022-06-12 04:55:08.620050: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f096ae4dc00 of size 147456 next 14\n",
      "2022-06-12 04:55:08.620055: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7f096ae71c00 of size 294912 next 19\n",
      "2022-06-12 04:55:08.620060: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f096aeb9c00 of size 294912 next 18\n",
      "2022-06-12 04:55:08.620065: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f096af01c00 of size 779264 next 18446744073709551615\n",
      "2022-06-12 04:55:08.620070: I tensorflow/core/common_runtime/bfc_allocator.cc:1071]      Summary of in-use Chunks by size: \n",
      "2022-06-12 04:55:08.620077: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 15 Chunks of size 256 totalling 3.8KiB\n",
      "2022-06-12 04:55:08.620083: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 512 totalling 512B\n",
      "2022-06-12 04:55:08.620089: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 1280 totalling 1.2KiB\n",
      "2022-06-12 04:55:08.620094: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 6912 totalling 6.8KiB\n",
      "2022-06-12 04:55:08.620100: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 147456 totalling 144.0KiB\n",
      "2022-06-12 04:55:08.620106: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 294912 totalling 288.0KiB\n",
      "2022-06-12 04:55:08.620112: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 779264 totalling 761.0KiB\n",
      "2022-06-12 04:55:08.620118: I tensorflow/core/common_runtime/bfc_allocator.cc:1078] Sum Total of in-use chunks: 1.18MiB\n",
      "2022-06-12 04:55:08.620123: I tensorflow/core/common_runtime/bfc_allocator.cc:1080] total_region_allocated_bytes_: 1835008 memory_limit_: 1835008 available bytes: 0 curr_region_allocation_bytes_: 3670016\n",
      "2022-06-12 04:55:08.620133: I tensorflow/core/common_runtime/bfc_allocator.cc:1086] Stats: \n",
      "Limit:                         1835008\n",
      "InUse:                         1234176\n",
      "MaxInUse:                      1234176\n",
      "NumAllocs:                          30\n",
      "MaxAllocSize:                   779264\n",
      "Reserved:                            0\n",
      "PeakReserved:                        0\n",
      "LargestFreeBlock:                    0\n",
      "\n",
      "2022-06-12 04:55:08.620152: W tensorflow/core/common_runtime/bfc_allocator.cc:474] **_______________*********_______________*************************************************xxxxxxxxxx\n",
      "2022-06-12 04:55:08.620214: W tensorflow/core/framework/op_kernel.cc:1733] RESOURCE_EXHAUSTED: failed to allocate memory\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "failed to allocate memory [Op:Mul]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_20680/3593349269.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m     \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m     \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_20680/3593349269.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;31m# set model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mTEST_CHECKPOINT_PATH_N_FILE_NAME\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'../../../raw_data/checkpoints/test_220612_checkpoint_VGG16_Dice.h5'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_20680/3593349269.py\u001b[0m in \u001b[0;36mset_model\u001b[0;34m(self, loss)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mgetVGG16\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSMR_Model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mUNET_INPUT_SHAPE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetVGG16\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_latest_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0;31m# see compile in SMR_Model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_20680/1279271435.py\u001b[0m in \u001b[0;36mget_latest_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_latest_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_vgg16_unet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_20680/1279271435.py\u001b[0m in \u001b[0;36mbuild_vgg16_unet\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;31m# see actual VGG-16 here: https://github.com/keras-team/keras/blob/v2.9.0/keras/applications/vgg16.py#L43-L227\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0mvgg16\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVGG16\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minclude_top\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'imagenet'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0;31m# vgg16.summary()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mvgg16\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/keras/applications/vgg16.py\u001b[0m in \u001b[0;36mVGG16\u001b[0;34m(include_top, weights, input_tensor, input_shape, pooling, classes, classifier_activation)\u001b[0m\n\u001b[1;32m    153\u001b[0m       128, (3, 3), activation='relu', padding='same', name='block2_conv1')(x)\n\u001b[1;32m    154\u001b[0m   x = layers.Conv2D(\n\u001b[0;32m--> 155\u001b[0;31m       128, (3, 3), activation='relu', padding='same', name='block2_conv2')(x)\n\u001b[0m\u001b[1;32m    156\u001b[0m   \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaxPooling2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'block2_pool'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/keras/backend.py\u001b[0m in \u001b[0;36mrandom_uniform\u001b[0;34m(self, shape, minval, maxval, dtype)\u001b[0m\n\u001b[1;32m   1920\u001b[0m     return tf.random.uniform(\n\u001b[1;32m   1921\u001b[0m         \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mminval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaxval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1922\u001b[0;31m         seed=self.make_legacy_seed())\n\u001b[0m\u001b[1;32m   1923\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1924\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mtruncated_normal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstddev\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: failed to allocate memory [Op:Mul]"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# from lwb_smr.CustomDataLoader import CustomDataLoader\n",
    "# from lwb_smr.model import SMR_Model\n",
    "# from lwb_smr.utils import PushMLFlow\n",
    "\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "# from tensorflow.keras.metrics import AUC, IoU\n",
    "\n",
    "        \n",
    "def dice_loss(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.math.sigmoid(y_pred)\n",
    "    numerator = 2 * tf.reduce_sum(y_true * y_pred)\n",
    "    denominator = tf.reduce_sum(y_true + y_pred)\n",
    "\n",
    "    return 1 - numerator / denominator\n",
    "\n",
    "class Trainer():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def set_model(self, loss=dice_loss):\n",
    "        self.loss = loss\n",
    "        # Instantiate Model\n",
    "        # our_input_shape = (224,224,3)\n",
    "\n",
    "        getVGG16 = SMR_Model(UNET_INPUT_SHAPE)\n",
    "        self.model = getVGG16.get_latest_model()\n",
    "        # see compile in SMR_Model\n",
    "\n",
    "    def start_mlflow(self):\n",
    "        p = PushMLFlow(EXPERIMENT_NAME, EXPERIMENT_TAGS)\n",
    "        return p # returns a class instance of PushMLFlow\n",
    "    \n",
    "    def run(self):\n",
    "\n",
    "        print(80*'-')\n",
    "        print('------SETTING FOR DATA RUN------')\n",
    "\n",
    "        ### \n",
    "        ##\n",
    "        #\n",
    "        # customdata = self.just_get_the_data_loaded()\n",
    "\n",
    "        print(80*'-')\n",
    "        print('------MODEL RUNNING------')\n",
    "\n",
    "        # set mflow      \n",
    "        self.MFLOW = self.start_mlflow() # class instance of MLFLOW\n",
    "\n",
    "        \n",
    "        # set model\n",
    "        self.set_model()\n",
    "        \n",
    "        TEST_CHECKPOINT_PATH_N_FILE_NAME = '../../../raw_data/checkpoints/test_220612_checkpoint_VGG16_Dice.h5'\n",
    "        TEST_MODEL_PATH_N_FILE_NAME = '../../../raw_data/models/test_220612_UNET_VGG16_Dice_224x224x3.h5'\n",
    "\n",
    "        mc = ModelCheckpoint(TEST_CHECKPOINT_PATH_N_FILE_NAME, save_best_only=True) # could put path here\n",
    "        es = EarlyStopping(patience=15, restore_best_weights=True)\n",
    "        self.history = self.model.fit(\n",
    "            ds_train,\n",
    "            validation_data=ds_val,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            epochs=EPOCHS,\n",
    "            callbacks=[mc, es]\n",
    "            )\n",
    "\n",
    "        # model_path_and_filename = 'test_220612_UNET_VGG16_Dice_224x224x3.h5'\n",
    "        self.model.save(TEST_MODEL_PATH_N_FILE_NAME)\n",
    "\n",
    "        self.MFLOW.mlflow_log_param('loss', self.loss)\n",
    "\n",
    "        print(80*'=')\n",
    "        print('------MODEL RUN SUCCESFULLY COMPLETED------')\n",
    "\n",
    "        self.evaluate()\n",
    "\n",
    "    def evaluate(self):\n",
    "        print(80*'-')\n",
    "        print('------MODEL EVALUATING------')\n",
    "        results = self.model.evaluate(ds_val)\n",
    "        for result in results:\n",
    "            self.MFLOW.mlflow_log_metric('metric X val output', result)\n",
    "        print(80*'=')\n",
    "        print('------MODEL EVALUATED------')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    pass\n",
    "    t = Trainer()\n",
    "    t.run()\n",
    "    t.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633bd30b-3127-4f24-8ade-052e58ad8051",
   "metadata": {},
   "outputs": [],
   "source": [
    "########\n",
    "####\n",
    "###\n",
    "#\n",
    "#        NOTES:  \n",
    "#                  \n",
    "#\n",
    "##\n",
    "###\n",
    "####\n",
    "########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6fd1ff-e898-4563-8f26-d622e1b26976",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00435da1-80f0-48dd-ba29-8dc7c83ffb37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-8.m93",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-8:m93"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
