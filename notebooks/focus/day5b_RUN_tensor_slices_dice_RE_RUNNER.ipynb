{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f260c50-8b65-4279-9362-e345f624d1ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MLFLOW_URI = \"https://mlflow.lewagon.ai/\"\n",
    "EXPERIMENT_NAME = \"UK Lon lwb_smr vertex_run_02\" # template\n",
    "EXPERIMENT_TAGS = {\n",
    "    'USER': 'hsth',\n",
    "    'RUN NAME': 'vertex2, operational',\n",
    "    'VERSION': 'M2_R04_15',\n",
    "    'DESCRIPTION': 'Model VGG16 UNet, 20+now another 30 epochs, 72k images',\n",
    "    'LOSS': 'dice',\n",
    "    'METRICS': 'accuracy, binaryIoU, AUC'\n",
    "}\n",
    "\n",
    "UNET_INPUT_SHAPE = (224,224,3)\n",
    "BATCH_SIZE = 60\n",
    "EPOCHS = 30\n",
    "# LOSS='binary_crossentropy'\n",
    "LOSS = 'DICE'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2d074b-ef96-4cd9-a984-d8ae94a895e9",
   "metadata": {},
   "source": [
    "# TENSOR SLICE DATA LOADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c942a20-9b3f-4303-9a83-4e141510352f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from lwb_smr.CustomDataLoader import CustomDataLoader\n",
    "from lwb_smr.data import LoadDataSets\n",
    "import os\n",
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from PIL import Image, ImageDraw\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf600518-8e02-4241-b689-c1e4ebb6a386",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoints\t\tmodels\t\t      train_mask_tiles_jpeg\n",
      "image_datasets_csv\tmodels_a\t      vertex_data_jpegs.zip\n",
      "image_datasets_csv.zip\ttrain_RGB_tiles_jpeg\n"
     ]
    }
   ],
   "source": [
    "!ls '../../../raw_data/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f1bfa94-84a3-4fcf-9cb1-dfed162302d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "x_path = '../../../raw_data/train_RGB_tiles_jpeg/'\n",
    "x_images = os.listdir(x_path)\n",
    "y_path = '../../../raw_data/train_mask_tiles_jpeg/'\n",
    "y_masks = os.listdir(y_path)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b1e7035-69de-469b-ae03-ee3a58096be1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../../../raw_data/train_RGB_tiles_jpeg',\n",
       " '../../../raw_data/train_mask_tiles_jpeg']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root_path = '../../../raw_data/'\n",
    "folders = ['train_RGB_tiles_jpeg', 'train_mask_tiles_jpeg']\n",
    "folder_path = [f'{root_path}{folder}' for folder in folders]\n",
    "folder_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4bb5bde4-64e3-4cc3-8319-c1e2efcce390",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images, train_mask = [], []\n",
    "train_images =[f'../../../raw_data/train_RGB_tiles_jpeg/{filename}' for filename in os.listdir(folder_path[0])]\n",
    "# for i, filename in enumerate(os.listdir(folder_path[0])):\n",
    "#     if i == 2*BATCH_SIZE: break\n",
    "#     train_images.append(f'../../../raw_data/train_RGB_tiles_jpeg/{filename}')\n",
    "    \n",
    "train_mask = [f'../../../raw_data/train_mask_tiles_jpeg/{filename}' for filename in os.listdir(folder_path[1])]\n",
    "# for i, filename in enumerate(os.listdir(folder_path[1])):\n",
    "#     if i == 2*BATCH_SIZE: break\n",
    "#     train_mask.append(f'../../../raw_data/train_mask_tiles_jpeg/{filename}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95e5a4c1-57e8-4dad-90f4-58f9ec6edcef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(72000,\n",
       "                                           image_path  \\\n",
       " 0  ../../../raw_data/train_RGB_tiles_jpeg/austin1...   \n",
       " 1  ../../../raw_data/train_RGB_tiles_jpeg/austin1...   \n",
       " 2  ../../../raw_data/train_RGB_tiles_jpeg/austin1...   \n",
       " 3  ../../../raw_data/train_RGB_tiles_jpeg/austin1...   \n",
       " 4  ../../../raw_data/train_RGB_tiles_jpeg/austin1...   \n",
       " \n",
       "                                            mask_path  \n",
       " 0  ../../../raw_data/train_mask_tiles_jpeg/austin...  \n",
       " 1  ../../../raw_data/train_mask_tiles_jpeg/austin...  \n",
       " 2  ../../../raw_data/train_mask_tiles_jpeg/austin...  \n",
       " 3  ../../../raw_data/train_mask_tiles_jpeg/austin...  \n",
       " 4  ../../../raw_data/train_mask_tiles_jpeg/austin...  )"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images.sort()\n",
    "train_mask.sort()\n",
    "\n",
    "\n",
    "train_df = pd.DataFrame()\n",
    "# train_df['file_path'] = train_images\n",
    "train_df['image_path'] = train_images\n",
    "train_df['mask_path'] = train_mask\n",
    "\n",
    "len(train_df), train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20b3769d-24d9-4378-97e2-28ae1ee6e243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef7e9acb-ef27-49fe-b037-93e9ae4e6990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# root_path = '/raw_data/'\n",
    "\n",
    "# # raw_data/train_mask_tiles_jpeg\n",
    "\n",
    "# x_path = root_path + 'train_RGB_tiles_jpeg/'\n",
    "# x_images = os.listdir(x_path)\n",
    "# y_path = root_path + 'train_mask_tiles_jpeg/'\n",
    "# y_masks = os.listdir(y_path)\n",
    "    \n",
    "\n",
    "# folders = ['train_RGB_tiles_jpeg', 'train_mask_tiles_jpeg']\n",
    "# folder_path = [f'{root_path}{folder}' for folder in folders]\n",
    "# folder_path\n",
    "\n",
    "# train_images, train_mask = [], []\n",
    "# train_images = [f'{x_images}{filename}' for filename in os.listdir(folder_path[0])]\n",
    "# train_mask = [f'{y_masks}{filename}' for filename in os.listdir(folder_path[1])]\n",
    "\n",
    "# train_images.sort()\n",
    "# train_mask.sort()\n",
    "\n",
    "\n",
    "# train_df = pd.DataFrame()\n",
    "# # train_df['file_path'] = train_images\n",
    "# train_df['image_path'] = train_images\n",
    "# train_df['mask_path'] = train_mask\n",
    "\n",
    "# train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "237149c5-1be0-4939-a96e-b7709a28fef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-12 14:06:05.505342: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-12 14:06:05.517347: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-12 14:06:05.517969: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-12 14:06:05.519401: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-12 14:06:05.521114: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-12 14:06:05.521864: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-12 14:06:05.522596: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-12 14:06:06.024255: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-12 14:06:06.024984: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-12 14:06:06.025635: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-12 14:06:06.026214: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13823 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def holdout(df, train_ratio=0.8, test_to_val_ratio=0.5, include_all=False):\n",
    "\n",
    "    img_paths = df[\"image_path\"].values\n",
    "    msk_paths = df[\"mask_path\"].values\n",
    "\n",
    "    df_mask = df.copy()\n",
    "\n",
    "    df_train, df_val = train_test_split(df_mask, train_size=train_ratio)\n",
    "    df_test, df_val = train_test_split(df_val, test_size=test_to_val_ratio)\n",
    "\n",
    "    ds_train = tf.data.Dataset.from_tensor_slices(\n",
    "         (df_train[\"image_path\"].values, df_train[\"mask_path\"].values)\n",
    "    )\n",
    "    ds_val = tf.data.Dataset.from_tensor_slices(\n",
    "        (df_val[\"image_path\"].values, df_val[\"mask_path\"].values)\n",
    "    )\n",
    "    ds_test = tf.data.Dataset.from_tensor_slices(\n",
    "        (df_test[\"image_path\"].values, df_test[\"mask_path\"].values)\n",
    "    )\n",
    "\n",
    "    return ds_train, ds_val, ds_test\n",
    "\n",
    "ds_train, ds_val, ds_test = holdout(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "052c664a-fcc9-4d0d-b32b-354c71c72f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_path(input_path, mask_path):\n",
    "    \"\"\"\n",
    "    Load images from files.\n",
    "    :input_path: the path to the satellite file\n",
    "    :mask_path: the path to the mask file\n",
    "    :return: The image and mask\n",
    "    .. note:: Works with jpg images \n",
    "              Only the first channel is kept for the mask\n",
    "    \"\"\"\n",
    "    \n",
    "    IMAGE_SQ_SIZE = 224\n",
    "\n",
    "    input_img = tf.io.read_file(input_path)   \n",
    "    input_img = tf.io.decode_jpeg(input_img, channels=3)\n",
    "    input_img =  tf.image.resize(input_img, [IMAGE_SQ_SIZE, IMAGE_SQ_SIZE])\n",
    "\n",
    "    mask_img = tf.io.read_file(mask_path)   \n",
    "    mask_img = tf.io.decode_jpeg(mask_img, channels=1)\n",
    "    mask_img =  tf.image.resize(mask_img, [IMAGE_SQ_SIZE, IMAGE_SQ_SIZE])\n",
    "\n",
    "   \n",
    "    return input_img, mask_img\n",
    "\n",
    "def normalize(image, mask):\n",
    "    # image = tf.cast(image, tf.float32) / 255.\n",
    "\n",
    "    return tf.math.divide(image, 255), tf.math.divide(mask, 255) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "480522e8-7f1d-427b-a957-19adf478be05",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "ds_train = ds_train.map(process_path) \\\n",
    ".map(normalize) \\\n",
    ".batch(batch_size=BATCH_SIZE) \\\n",
    ".prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "ds_val = ds_val.map(process_path) \\\n",
    ".map(normalize) \\\n",
    ".batch(batch_size=BATCH_SIZE) \\\n",
    ".prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7c093c-c72a-4859-b6c4-b57dd9ba9cf9",
   "metadata": {},
   "source": [
    "### we do not use:\n",
    "# CustomDataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7d74db3d-59f2-48b4-b631-0a60de6bfa49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7925488-670a-4d53-97e9-fde6788e6c0d",
   "metadata": {},
   "source": [
    "# model.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385f7a29-de1c-4e54-83ec-15991eef2155",
   "metadata": {},
   "source": [
    "#### SMR_Model below, but these cells load the model:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50783bf-2ec4-4115-a50b-f37997b9d5f1",
   "metadata": {
    "tags": []
   },
   "source": [
    "# LOAD THE MODEL SPECIFIED:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "526ed305-5b58-42ea-bd88-80dd08a88dc4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model_load_string # from cells above... RENAMED BELOW ON GCP..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "95ae3d4e-2abf-4167-9a40-949ca58a0f28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def dice_loss(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.math.sigmoid(y_pred)\n",
    "    numerator = 2 * tf.reduce_sum(y_true * y_pred)\n",
    "    denominator = tf.reduce_sum(y_true + y_pred)\n",
    "\n",
    "    return 1 - numerator / denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7cf1b1d4-1d9c-4226-b70d-213c6aa8f36f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # WORKS WELL FOR LOCAL LOAD:\n",
    "# from tensorflow.keras.models import Model, Sequential\n",
    "\n",
    "# custom_objects_dict = {\n",
    "#                 'dice_loss': dice_loss\n",
    "# }\n",
    "# loaded_model = tf.keras.models.load_model(model_load_string, custom_objects=custom_objects_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab1b2f7-1322-4b47-a22c-eadb0dce81e0",
   "metadata": {},
   "source": [
    "## load direct from GCP bucket:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "51c910c4-731a-4d5e-ab14-09c8db231468",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# NOT THIS ONE!!!!   \n",
    "#               LOADING_MODEL_GSUTIL = 'gs://lwb-solar-my-roof/models/220610_full_model_vgg16_10e_20e_more.h5'\n",
    "# THIS ONE >>>>>>> \n",
    "#               i.e. with the dice loss\n",
    "LOADING_MODEL_GSUTIL = 'gs://lwb-solar-my-roof/models/220611_VGG16_Dice_20e_in_shape_224x224x3.h5'\n",
    "\n",
    "custom_objects_dict = {\n",
    "                'dice_loss': dice_loss\n",
    "}\n",
    "loaded_model = tf.keras.models.load_model(LOADING_MODEL_GSUTIL, custom_objects=custom_objects_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "731f36e9-cebb-485a-9bda-e1084f589fef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"first_VGG16_UNET\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 224, 224, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " block1_conv1 (Conv2D)          (None, 224, 224, 64  1792        ['input_1[0][0]']                \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " block1_conv2 (Conv2D)          (None, 224, 224, 64  36928       ['block1_conv1[0][0]']           \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " block1_pool (MaxPooling2D)     (None, 112, 112, 64  0           ['block1_conv2[0][0]']           \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " block2_conv1 (Conv2D)          (None, 112, 112, 12  73856       ['block1_pool[0][0]']            \n",
      "                                8)                                                                \n",
      "                                                                                                  \n",
      " block2_conv2 (Conv2D)          (None, 112, 112, 12  147584      ['block2_conv1[0][0]']           \n",
      "                                8)                                                                \n",
      "                                                                                                  \n",
      " block2_pool (MaxPooling2D)     (None, 56, 56, 128)  0           ['block2_conv2[0][0]']           \n",
      "                                                                                                  \n",
      " block3_conv1 (Conv2D)          (None, 56, 56, 256)  295168      ['block2_pool[0][0]']            \n",
      "                                                                                                  \n",
      " block3_conv2 (Conv2D)          (None, 56, 56, 256)  590080      ['block3_conv1[0][0]']           \n",
      "                                                                                                  \n",
      " block3_conv3 (Conv2D)          (None, 56, 56, 256)  590080      ['block3_conv2[0][0]']           \n",
      "                                                                                                  \n",
      " block3_pool (MaxPooling2D)     (None, 28, 28, 256)  0           ['block3_conv3[0][0]']           \n",
      "                                                                                                  \n",
      " block4_conv1 (Conv2D)          (None, 28, 28, 512)  1180160     ['block3_pool[0][0]']            \n",
      "                                                                                                  \n",
      " block4_conv2 (Conv2D)          (None, 28, 28, 512)  2359808     ['block4_conv1[0][0]']           \n",
      "                                                                                                  \n",
      " block4_conv3 (Conv2D)          (None, 28, 28, 512)  2359808     ['block4_conv2[0][0]']           \n",
      "                                                                                                  \n",
      " block4_pool (MaxPooling2D)     (None, 14, 14, 512)  0           ['block4_conv3[0][0]']           \n",
      "                                                                                                  \n",
      " block5_conv1 (Conv2D)          (None, 14, 14, 512)  2359808     ['block4_pool[0][0]']            \n",
      "                                                                                                  \n",
      " block5_conv2 (Conv2D)          (None, 14, 14, 512)  2359808     ['block5_conv1[0][0]']           \n",
      "                                                                                                  \n",
      " block5_conv3 (Conv2D)          (None, 14, 14, 512)  2359808     ['block5_conv2[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_transpose (Conv2DTransp  (None, 28, 28, 512)  1049088    ['block5_conv3[0][0]']           \n",
      " ose)                                                                                             \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 28, 28, 1024  0           ['conv2d_transpose[0][0]',       \n",
      "                                )                                 'block4_conv3[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 28, 28, 512)  4719104     ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 28, 28, 512)  2048       ['conv2d[0][0]']                 \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " activation (Activation)        (None, 28, 28, 512)  0           ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 28, 28, 512)  2359808     ['activation[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 28, 28, 512)  2048       ['conv2d_1[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_1 (Activation)      (None, 28, 28, 512)  0           ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_transpose_1 (Conv2DTran  (None, 56, 56, 256)  524544     ['activation_1[0][0]']           \n",
      " spose)                                                                                           \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 56, 56, 512)  0           ['conv2d_transpose_1[0][0]',     \n",
      "                                                                  'block3_conv3[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 56, 56, 256)  1179904     ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 56, 56, 256)  1024       ['conv2d_2[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_2 (Activation)      (None, 56, 56, 256)  0           ['batch_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)              (None, 56, 56, 256)  590080      ['activation_2[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 56, 56, 256)  1024       ['conv2d_3[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_3 (Activation)      (None, 56, 56, 256)  0           ['batch_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_transpose_2 (Conv2DTran  (None, 112, 112, 12  131200     ['activation_3[0][0]']           \n",
      " spose)                         8)                                                                \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate)    (None, 112, 112, 25  0           ['conv2d_transpose_2[0][0]',     \n",
      "                                6)                                'block2_conv2[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)              (None, 112, 112, 12  295040      ['concatenate_2[0][0]']          \n",
      "                                8)                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_4 (BatchNo  (None, 112, 112, 12  512        ['conv2d_4[0][0]']               \n",
      " rmalization)                   8)                                                                \n",
      "                                                                                                  \n",
      " activation_4 (Activation)      (None, 112, 112, 12  0           ['batch_normalization_4[0][0]']  \n",
      "                                8)                                                                \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)              (None, 112, 112, 12  147584      ['activation_4[0][0]']           \n",
      "                                8)                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_5 (BatchNo  (None, 112, 112, 12  512        ['conv2d_5[0][0]']               \n",
      " rmalization)                   8)                                                                \n",
      "                                                                                                  \n",
      " activation_5 (Activation)      (None, 112, 112, 12  0           ['batch_normalization_5[0][0]']  \n",
      "                                8)                                                                \n",
      "                                                                                                  \n",
      " conv2d_transpose_3 (Conv2DTran  (None, 224, 224, 64  32832      ['activation_5[0][0]']           \n",
      " spose)                         )                                                                 \n",
      "                                                                                                  \n",
      " concatenate_3 (Concatenate)    (None, 224, 224, 12  0           ['conv2d_transpose_3[0][0]',     \n",
      "                                8)                                'block1_conv2[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)              (None, 224, 224, 64  73792       ['concatenate_3[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_6 (BatchNo  (None, 224, 224, 64  256        ['conv2d_6[0][0]']               \n",
      " rmalization)                   )                                                                 \n",
      "                                                                                                  \n",
      " activation_6 (Activation)      (None, 224, 224, 64  0           ['batch_normalization_6[0][0]']  \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)              (None, 224, 224, 64  36928       ['activation_6[0][0]']           \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_7 (BatchNo  (None, 224, 224, 64  256        ['conv2d_7[0][0]']               \n",
      " rmalization)                   )                                                                 \n",
      "                                                                                                  \n",
      " activation_7 (Activation)      (None, 224, 224, 64  0           ['batch_normalization_7[0][0]']  \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)              (None, 224, 224, 1)  65          ['activation_7[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 25,862,337\n",
      "Trainable params: 11,143,809\n",
      "Non-trainable params: 14,718,528\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Check its architecture\n",
    "loaded_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1ab6d800-39ea-4458-bdcb-abff179bd1ed",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Activation, ReLU\n",
    "# from tensorflow.keras.layers import BatchNormalization, Conv2DTranspose, Concatenate\n",
    "# from tensorflow.keras.models import Model, Sequential\n",
    "# from tensorflow.keras.utils import plot_model\n",
    "\n",
    "# from tensorflow.keras.applications import VGG16\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# class SMR_Model():\n",
    "#     ''' creating our first lwb_smr models '''\n",
    "\n",
    "#     def __init__(self, input_shape):\n",
    "#         self.input_shape = input_shape\n",
    "\n",
    "#     def get_latest_model(self):\n",
    "#         model = self.build_vgg16_unet(self.input_shape)\n",
    "#         model = self.compile_model(model)\n",
    "\n",
    "#         return model\n",
    "\n",
    "#     def convolution_block(self, inputs, num_filters):\n",
    "#         ''' simple UNET convolution block with BatchNormalisation '''\n",
    "\n",
    "#         # convolution layer 1 of the block\n",
    "#         x = Conv2D(num_filters, (3,3), padding='same')(inputs)  # padding='same' to avoid cut-down with conv\n",
    "#         x = BatchNormalization()(x)\n",
    "#         x = Activation('relu')(x)\n",
    "\n",
    "#         # convolution layer 2 of the block\n",
    "#         x = Conv2D(num_filters, (3,3), padding='same')(x)\n",
    "#         x = BatchNormalization()(x)\n",
    "#         x = Activation('relu')(x)\n",
    "\n",
    "#         # max pooling not used here as just the bridge\n",
    "\n",
    "#         return x\n",
    "\n",
    "#     def decoder_block(self, inputs, skip_tensor, num_filters):\n",
    "#         ''' decoder block for UNET '''\n",
    "#         # adds in the skips with concatenate\n",
    "#         x = Conv2DTranspose(num_filters, (2,2), strides=2, padding='same')(inputs) # stride important here to up-sample\n",
    "#         x = Concatenate()([x, skip_tensor])     # bringing in skip layer\n",
    "#         x = self.convolution_block(x, num_filters)\n",
    "\n",
    "#         return x\n",
    "\n",
    "#     def build_vgg16_unet(self, input_shape):\n",
    "#         ''' build vgg-16 '''\n",
    "\n",
    "#         inputs = Input(input_shape)\n",
    "\n",
    "#         # see actual VGG-16 here: https://github.com/keras-team/keras/blob/v2.9.0/keras/applications/vgg16.py#L43-L227\n",
    "#         vgg16 = VGG16(include_top=False, weights='imagenet', input_tensor=inputs)\n",
    "#         # vgg16.summary()\n",
    "#         vgg16.trainable = False\n",
    "\n",
    "#         ''' Encoder - skip layers '''\n",
    "#         skip1 = vgg16.get_layer('block1_conv2').output #  256 x 256, 64 filters in vgg16\n",
    "#         skip2 = vgg16.get_layer('block2_conv2').output #  128 x 128, 128 filters in vgg16\n",
    "#         skip3 = vgg16.get_layer('block3_conv3').output #   64 x 64, 256 filters in vgg16\n",
    "#         skip4 = vgg16.get_layer('block4_conv3').output #   32 x 32, 512 filters in vgg16\n",
    "#         # display('skip4: ' + str(skip4.shape))\n",
    "\n",
    "#         # only need to specify the skip layers, as VGG16 is an Encoder\n",
    "#         # Therefore, VGG16 comes built with MaxPool2d, so we don't specify\n",
    "\n",
    "#         ''' Bridge '''\n",
    "#         bridge = vgg16.get_layer('block5_conv3').output # 16 x 16, with 512 filters in vgg16\n",
    "#         # display('bridge: ' + str(bridge.shape))\n",
    "\n",
    "\n",
    "#         ''' Decoder '''\n",
    "#         d1 = self.decoder_block(bridge, skip4, 512) #  512 filters, as per the bridge\n",
    "#         d2 = self.decoder_block(d1, skip3, 256) #  256 filters\n",
    "#         d3 = self.decoder_block(d2, skip2, 128) #  128 filters\n",
    "#         d4 = self.decoder_block(d3, skip1, 64)  #   64 filters\n",
    "\n",
    "#         ''' Output '''\n",
    "#         outputs = Conv2D(1, (1,1), padding='same', activation='sigmoid')(d4)\n",
    "\n",
    "#         model = Model(inputs, outputs, name='first_VGG16_UNET')\n",
    "\n",
    "#         return model\n",
    "    \n",
    "#     def dice_loss(self, y_true, y_pred):\n",
    "#         y_true = tf.cast(y_true, tf.float32)\n",
    "#         y_pred = tf.math.sigmoid(y_pred)\n",
    "#         numerator = 2 * tf.reduce_sum(y_true * y_pred)\n",
    "#         denominator = tf.reduce_sum(y_true + y_pred)\n",
    "\n",
    "#         return 1 - numerator / denominator\n",
    "\n",
    "\n",
    "#     def compile_model(self, m):\n",
    "#         ''' with accuracy, binaryIoU, AuC '''\n",
    "#         # metrics\n",
    "#         threshold = 0.5\n",
    "#         binaryIoU = tf.keras.metrics.BinaryIoU(target_class_ids=[1], threshold=threshold)\n",
    "#         AuC = tf.keras.metrics.AUC()\n",
    "\n",
    "#         # loss\n",
    "#         #self.dice_loss = ...\n",
    "        \n",
    "#         # Compile Model\n",
    "#         m.compile(\n",
    "#                     loss=self.dice_loss,\n",
    "#                     optimizer='adam',\n",
    "#                     metrics=['accuracy', binaryIoU, AuC]\n",
    "#                     )\n",
    "#         return m\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e02df14-1730-4a67-84d9-6df7918d7cfe",
   "metadata": {
    "tags": []
   },
   "source": [
    "# utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c9f96beb-091b-4ebf-815a-26b02d7ea818",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# mlflow\n",
    "#\n",
    "# and others...\n",
    "\n",
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "from memoized_property import memoized_property\n",
    "\n",
    "MLFLOW_URI = \"https://mlflow.lewagon.ai/\"\n",
    "\n",
    "class PushMLFlow():\n",
    "    '''\n",
    "        MLFLOW_URI = \"https://mlflow.lewagon.ai/\"\n",
    "        EXPERIMENT_NAME = \"[UK] [LONDON] [SOLAR_ROOF] TEST RUN\" # template\n",
    "        EXPERIMENT_TAGS = {\n",
    "            'USER': 'test_user',\n",
    "            'RUN NAME': 'test runs',\n",
    "            'VERSION': '1.0.1',\n",
    "            'LOSS': 'dice'\n",
    "            'DESCRIPTION': 'testing MLFlow Pipeline. Model - basic U-Net structure, 2 epochs, 15 images'\n",
    "        }\n",
    "    '''\n",
    "\n",
    "    def __init__(self, experiment_name, experiment_tags):\n",
    "        self.experiment_name = experiment_name\n",
    "        self.experiment_tag = experiment_tags\n",
    "\n",
    "    @memoized_property\n",
    "    def mlflow_client(self):\n",
    "        mlflow.set_tracking_uri(MLFLOW_URI)\n",
    "        return MlflowClient()\n",
    "\n",
    "    @memoized_property\n",
    "    def mlflow_experiment_id(self):\n",
    "        try:\n",
    "            return self.mlflow_client.create_experiment(self.experiment_name)\n",
    "        except BaseException:\n",
    "            return self.mlflow_client.get_experiment_by_name(self.experiment_name).experiment_id\n",
    "\n",
    "    @memoized_property\n",
    "    def mlflow_run(self):\n",
    "        return self.mlflow_client.create_run(self.mlflow_experiment_id, tags=self.experiment_tag)\n",
    "\n",
    "    def mlflow_log_param(self, key, value):\n",
    "        self.mlflow_client.log_param(self.mlflow_run.info.run_id, key, value)\n",
    "\n",
    "    def mlflow_log_metric(self, key, value):\n",
    "        self.mlflow_client.log_metric(self.mlflow_run.info.run_id, key, value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3d60ba85-ce10-431b-b21a-dd584f8a4a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6352a612-b695-41e5-83a0-828ecdd13dc8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "------SETTING FOR DATA RUN------\n",
      "--------------------------------------------------------------------------------\n",
      "------MODEL RUNNING------\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-12 14:06:35.175788: I tensorflow/stream_executor/cuda/cuda_dnn.cc:368] Loaded cuDNN version 8200\n",
      "2022-06-12 14:06:56.926811: W tensorflow/core/common_runtime/bfc_allocator.cc:275] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.92GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n",
      "2022-06-12 14:06:56.926880: W tensorflow/core/common_runtime/bfc_allocator.cc:275] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.92GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# from lwb_smr.CustomDataLoader import CustomDataLoader\n",
    "# from lwb_smr.model import SMR_Model\n",
    "# from lwb_smr.utils import PushMLFlow\n",
    "\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "# from tensorflow.keras.metrics import AUC, IoU\n",
    "\n",
    "        \n",
    "def dice_loss(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.math.sigmoid(y_pred)\n",
    "    numerator = 2 * tf.reduce_sum(y_true * y_pred)\n",
    "    denominator = tf.reduce_sum(y_true + y_pred)\n",
    "\n",
    "    return 1 - numerator / denominator\n",
    "\n",
    "class Trainer():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def set_model(self, loss=dice_loss):\n",
    "        self.loss = loss\n",
    "        # Instantiate Model\n",
    "        # our_input_shape = (224,224,3)\n",
    "\n",
    "        #### getVGG16 = SMR_Model(UNET_INPUT_SHAPE)\n",
    "        #### self.model = getVGG16.get_latest_model()\n",
    "        self.model = loaded_model # see additions above\n",
    "        # see compile in SMR_Model\n",
    "\n",
    "    def start_mlflow(self):\n",
    "        p = PushMLFlow(EXPERIMENT_NAME, EXPERIMENT_TAGS)\n",
    "        return p # returns a class instance of PushMLFlow\n",
    "    \n",
    "    def run(self):\n",
    "\n",
    "        print(80*'-')\n",
    "        print('------SETTING FOR DATA RUN------')\n",
    "\n",
    "        ### \n",
    "        ##\n",
    "        #\n",
    "        # customdata = self.just_get_the_data_loaded()\n",
    "\n",
    "        print(80*'-')\n",
    "        print('------MODEL RUNNING------')\n",
    "\n",
    "        # set mflow      \n",
    "        self.MFLOW = self.start_mlflow() # class instance of MLFLOW\n",
    "\n",
    "        \n",
    "        # set model\n",
    "        self.set_model()\n",
    "\n",
    "        mc = ModelCheckpoint('220612_checkpoint_VGG16_Dice.h5', save_best_only=True) # could put path here\n",
    "        es = EarlyStopping(patience=15, restore_best_weights=True)\n",
    "        self.history = self.model.fit(\n",
    "            ds_train,\n",
    "            validation_data=ds_val,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            epochs=EPOCHS,\n",
    "            callbacks=[mc, es]\n",
    "            )\n",
    "\n",
    "        model_path_and_filename = '220612_VGG16_Dice_20e_now30e_more_in_shape_224x224x3.h5'\n",
    "        self.model.save(model_path_and_filename)\n",
    "\n",
    "        self.MFLOW.mlflow_log_param('loss', self.loss)\n",
    "\n",
    "        print(80*'=')\n",
    "        print('------MODEL RUN SUCCESFULLY COMPLETED------')\n",
    "\n",
    "        self.evaluate()\n",
    "\n",
    "    def evaluate(self):\n",
    "        print(80*'-')\n",
    "        print('------MODEL EVALUATING------')\n",
    "        results = self.model.evaluate(ds_val)\n",
    "        for result in results:\n",
    "            self.MFLOW.mlflow_log_metric('metric X val output', result)\n",
    "        print(80*'=')\n",
    "        print('------MODEL EVALUATED------')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    pass\n",
    "    t = Trainer()\n",
    "    t.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633bd30b-3127-4f24-8ade-052e58ad8051",
   "metadata": {},
   "outputs": [],
   "source": [
    "########\n",
    "####                        DICE             DICE             DICE             DICE             DICE             DICE             DICE             DICE\n",
    "###\n",
    "#\n",
    "#        NOTES: with DICE LOSS driving the model now (this is a sample run. \n",
    "#                                                                          Again, clear that binaryIoU metric is showing increases, as with all other metrics.\n",
    "#                                              (val) DICE LOSS reducing\n",
    "#                                              (val) ACCURACY increasing\n",
    "#                                              (val) binaryIoU increasing\n",
    "#                                              (val) AUC increasing\n",
    "#\n",
    "##\n",
    "###\n",
    "####\n",
    "########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6fd1ff-e898-4563-8f26-d622e1b26976",
   "metadata": {},
   "outputs": [],
   "source": [
    "t.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00435da1-80f0-48dd-ba29-8dc7c83ffb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "t.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab874103-2e4e-4d67-ad07-a47900ea8a5d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-8.m93",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-8:m93"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
